"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
"""

import asyncio
from news_crawler import crawl_naver_top_news_by_category

async def news_crawling():
    """ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ë„¤ì´ë²„ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    max_articles = int(input("ğŸ“ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘í•  ê¸°ì‚¬ ê°œìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (1-50): "))
    
    print(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ {max_articles}ê°œì”© ê¸°ì‚¬ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("-" * 60)
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
    result = await crawl_naver_top_news_by_category(max_articles_per_category=max_articles)
    
    if result['success']:
        print(f"âœ… ì„±ê³µì ìœ¼ë¡œ ì´ {result['articles_count']}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        print(f"ğŸ“Š ìˆ˜ì§‘ëœ ì¹´í…Œê³ ë¦¬: {result['categories_count']}ê°œ")
        print(f"âŒ ì—ëŸ¬ ìˆ˜: {result['errors_count']}ê°œ")
        
        print("\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:")
        print("-" * 50)
        for category, stats in result['category_stats'].items():
            print(f"  {category}:")
            print(f"    - URL ë°œê²¬: {stats['urls_found']}ê°œ")
            print(f"    - ê¸°ì‚¬ ì¶”ì¶œ: {stats['articles_extracted']}ê°œ")
            print(f"    - ì—ëŸ¬: {stats['errors']}ê°œ")
        
        print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼: {result['filename']}")
        
        print("\nğŸ“° ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìƒ˜í”Œ:")
        print("-" * 50)
        
        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê¸°ì‚¬ ì •ë¦¬
        by_category = {}
        for article in result['articles']:
            category = article['category']
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(article)
        
        for category, articles in by_category.items():
            print(f"\nğŸ·ï¸ {category} ({len(articles)}ê°œ):")
            for i, article in enumerate(articles[:3]):  # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ 3ê°œë§Œ í‘œì‹œ
                print(f"  {i+1}. {article['title'][:50]}...")
                print(f"     ì–¸ë¡ ì‚¬: {article['source']}")
                print(f"     ì‘ì„±ì: {article['author']}")
                print(f"     ë‚ ì§œ: {article['published_date']}")
        
        print(f"\nğŸ¯ ì „ì²´ ê¸°ì‚¬ ì¤‘ ì •ê·œí™” ì„±ê³µë¥ :")
        print("-" * 50)
        
        # ì •ê·œí™” ì„±ê³µë¥  ê³„ì‚°
        total_articles = len(result['articles'])
        with_date = len([a for a in result['articles'] if a['published_date']])
        with_author = len([a for a in result['articles'] if a['author']])
        with_source = len([a for a in result['articles'] if a['source']])
        
        print(f"  ë‚ ì§œ ì •ê·œí™”: {with_date}/{total_articles} ({with_date/total_articles*100:.1f}%)")
        print(f"  ì‘ì„±ì ì •ê·œí™”: {with_author}/{total_articles} ({with_author/total_articles*100:.1f}%)")
        print(f"  ì–¸ë¡ ì‚¬ ì¶”ì¶œ: {with_source}/{total_articles} ({with_source/total_articles*100:.1f}%)")
        
    else:
        print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
        print(f"ì—ëŸ¬: {result['errors']}")

if __name__ == "__main__":
    asyncio.run(news_crawling()) 