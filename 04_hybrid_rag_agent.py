#!/usr/bin/env python3
"""
Enhanced Hybrid RAG Agent: ChromaDB + Neo4j + LangGraph
ì‹¤ì œ ë‰´ìŠ¤ ì›ë³¸(ChromaDB) + ê´€ê³„ ê·¸ë˜í”„(Neo4j) ê²°í•© ì‹œìŠ¤í…œ
ìƒì„¸í•œ ê³¼ì • ì„¤ëª… ë° íŒë‹¨ ë¡œì§ íˆ¬ëª…ì„± ê°œì„ 
"""

from typing import TypedDict, List, Literal, Optional, Dict, Tuple
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from neo4j import GraphDatabase
import chromadb
import os
import json
import time
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

from langfuse.langchain import CallbackHandler
langfuse_handler = CallbackHandler()

# Enhanced State with detailed process tracking
class DetailedHybridRAGState(TypedDict):
    question: str
    # ê²€ìƒ‰ ê³¼ì • ìƒì„¸ ì¶”ì 
    search_strategy: Dict  # ê²€ìƒ‰ ì „ëµ ë° íŒë‹¨ ê³¼ì •
    chroma_process: Dict   # ChromaDB ê²€ìƒ‰ ìƒì„¸ ê³¼ì •
    neo4j_process: Dict    # Neo4j ê²€ìƒ‰ ìƒì„¸ ê³¼ì •
    quality_assessment: Dict  # í’ˆì§ˆ í‰ê°€ ê³¼ì •
    synthesis_process: Dict   # í†µí•© ê³¼ì •
    # ê²°ê³¼
    chroma_results: List[Dict]
    neo4j_results: List[Dict] 
    final_answer: str
    # ë©”íƒ€ ì •ë³´
    sources: List[str]
    confidence_score: float
    iteration_count: int
    execution_log: List[Dict]  # ì‹¤í–‰ ê³¼ì • ë¡œê·¸

class EnhancedHybridRAGAgent:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ëŠ” ë‚˜ì¤‘ì— ì´ˆê¸°í™”
        self.chroma_client = None
        self.news_collection = None
        
        # Neo4j ë“œë¼ì´ë²„
        self.neo4j_driver = self._get_neo4j_driver()
        
        # ì‹¤í–‰ ë¡œê·¸
        self.execution_log = []
    
    def _get_chroma_client(self):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì§€ì—° ì´ˆê¸°í™”"""
        if self.chroma_client is None:
            self.chroma_client = chromadb.PersistentClient(path="chroma_db_news_2")
            self.news_collection = self.chroma_client.get_collection("naver_news")
        return self.news_collection
    
    def _get_neo4j_driver(self):
        """Neo4j ì—°ê²°"""
        try:
            uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
            username = os.getenv("NEO4J_USERNAME", "neo4j")
            password = os.getenv("NEO4J_PASSWORD", "password")
            driver = GraphDatabase.driver(uri, auth=(username, password))
            return driver
        except Exception as e:
            print(f"Neo4j ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def log_step(self, step_name: str, details: Dict):
        """ì‹¤í–‰ ë‹¨ê³„ ë¡œê¹…"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "step": step_name,
            "details": details
        }
        self.execution_log.append(log_entry)

def analyze_question_strategy(state: DetailedHybridRAGState) -> DetailedHybridRAGState:
    """ì§ˆë¬¸ ë¶„ì„ ë° ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½"""
    agent = EnhancedHybridRAGAgent()
    question = state["question"]
    
    print(f"ğŸ¯ Step 1: ì§ˆë¬¸ ë¶„ì„ ë° ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½")
    print(f"ğŸ“ ì§ˆë¬¸: '{question}'")
    
    # LLMì„ ì‚¬ìš©í•œ ì§ˆë¬¸ ë¶„ì„
    analysis_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”:

ì§ˆë¬¸: {question}

ë¶„ì„ í•­ëª©:
1. ì§ˆë¬¸ ìœ í˜• (ì‚¬ì‹¤ í™•ì¸, ì¸ê³¼ê´€ê³„, í˜„í™© ë¶„ì„, ì˜ˆì¸¡ ë“±)
2. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (3-5ê°œ)
3. ì‹œê°„ì  ë²”ìœ„ (ìµœì‹ , íŠ¹ì • ê¸°ê°„, ì¼ë°˜ì )
4. ê²€ìƒ‰ ìš°ì„ ìˆœìœ„ (ë‰´ìŠ¤ ë‚´ìš© vs ê´€ê³„ ê·¸ë˜í”„)
5. ì˜ˆìƒ ë‹µë³€ ë³µì¡ë„ (ë‹¨ìˆœ/ë³µí•©)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "question_type": "ì§ˆë¬¸ ìœ í˜•",
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
    "time_scope": "ì‹œê°„ì  ë²”ìœ„",
    "search_priority": "chroma_first|neo4j_first|parallel",
    "complexity": "simple|complex",
    "reasoning": "ì „ëµ ì„ íƒ ì´ìœ "
}}
"""
    
    try:
        analysis_result = agent.llm.invoke(analysis_prompt, config={"callbacks": [langfuse_handler]}).content
        # JSON íŒŒì‹± ì‹œë„
        try:
            strategy = json.loads(analysis_result)
        except:
            # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì „ëµ
            strategy = {
                "question_type": "ì¼ë°˜ ì§ˆë¬¸",
                "keywords": question.split()[:3],
                "time_scope": "ìµœì‹ ",
                "search_priority": "parallel",
                "complexity": "complex",
                "reasoning": "ìë™ ë¶„ì„ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ì „ëµ ì ìš©"
            }
        
        print(f"  ğŸ“Š ì§ˆë¬¸ ìœ í˜•: {strategy['question_type']}")
        print(f"  ğŸ”‘ í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(strategy['keywords'])}")
        print(f"  â° ì‹œê°„ ë²”ìœ„: {strategy['time_scope']}")
        print(f"  ğŸ¯ ê²€ìƒ‰ ìš°ì„ ìˆœìœ„: {strategy['search_priority']}")
        print(f"  ğŸ§  ë³µì¡ë„: {strategy['complexity']}")
        print(f"  ğŸ’¡ ì „ëµ ê·¼ê±°: {strategy['reasoning']}")
        
        agent.log_step("question_analysis", strategy)
        
        return {
            "search_strategy": strategy,
            "execution_log": agent.execution_log
        }
        
    except Exception as e:
        print(f"  âŒ ì§ˆë¬¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
        default_strategy = {
            "question_type": "ì¼ë°˜ ì§ˆë¬¸",
            "keywords": question.split()[:3],
            "time_scope": "ìµœì‹ ",
            "search_priority": "parallel",
            "complexity": "complex",
            "reasoning": "ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ê¸°ë³¸ ì „ëµ ì ìš©"
        }
        return {
            "search_strategy": default_strategy,
            "execution_log": []
        }

def enhanced_chroma_search_node(state: DetailedHybridRAGState) -> DetailedHybridRAGState:
    """ê°•í™”ëœ ChromaDB ê²€ìƒ‰ (ìƒì„¸ ê³¼ì • ì¶”ì )"""
    agent = EnhancedHybridRAGAgent()
    question = state["question"]
    strategy = state.get("search_strategy", {})
    keywords = strategy.get("keywords", question.split())
    
    print(f"\nğŸ” Step 2: ChromaDB ë‰´ìŠ¤ ê²€ìƒ‰ (ìƒì„¸ ê³¼ì •)")
    print(f"ğŸ“‹ ê²€ìƒ‰ ì „ëµ: {strategy.get('search_priority', 'parallel')}")
    print(f"ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(keywords)}")
    
    chroma_process = {
        "start_time": time.time(),
        "search_method": "semantic_embedding",
        "keywords_used": keywords,
        "steps": []
    }
    
    try:
        # Step 1: ì„ë² ë”© ìƒì„±
        print(f"  ğŸ§® Step 2.1: ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì¤‘...")
        start_embed = time.time()
        query_embedding = agent.embeddings.embed_query(question)
        embed_time = time.time() - start_embed
        
        chroma_process["steps"].append({
            "step": "embedding_generation",
            "time_taken": embed_time,
            "embedding_dim": len(query_embedding),
            "success": True
        })
        
        print(f"    âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ (ì°¨ì›: {len(query_embedding)}, ì†Œìš”ì‹œê°„: {embed_time:.2f}ì´ˆ)")
        
        # Step 2: ChromaDB ê²€ìƒ‰ ì‹¤í–‰
        print(f"  ğŸ” Step 2.2: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤í–‰...")
        start_search = time.time()
        
        news_collection = agent._get_chroma_client()
        results = news_collection.query(
            query_embeddings=[query_embedding],
            n_results=10,  # ë” ë§ì€ ê²°ê³¼ ìš”ì²­ í›„ í•„í„°ë§
            include=["documents", "metadatas", "distances"]
        )
        
        search_time = time.time() - start_search
        
        chroma_process["steps"].append({
            "step": "semantic_search",
            "time_taken": search_time,
            "raw_results_count": len(results["documents"][0]) if results["documents"] else 0,
            "success": True
        })
        
        print(f"    âœ… ê²€ìƒ‰ ì™„ë£Œ (ì›ë³¸ ê²°ê³¼: {len(results['documents'][0]) if results['documents'] else 0}ê°œ, ì†Œìš”ì‹œê°„: {search_time:.2f}ì´ˆ)")
        
        # Step 3: ê²°ê³¼ í’ˆì§ˆ í‰ê°€ ë° í•„í„°ë§
        print(f"  ğŸ“Š Step 2.3: ê²°ê³¼ í’ˆì§ˆ í‰ê°€ ë° í•„í„°ë§...")
        
        chroma_results = []
        if results["documents"] and results["documents"][0]:
            for i, (doc, metadata, distance) in enumerate(zip(
                results["documents"][0],
                results["metadatas"][0], 
                results["distances"][0]
            )):
                relevance_score = 1 - distance
                
                # í’ˆì§ˆ í•„í„°ë§ (ê´€ë ¨ë„ ì„ê³„ê°’)
                if relevance_score > 0.3:  # ìµœì†Œ ê´€ë ¨ë„ ì„ê³„ê°’
                    # í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ê°€ ì ìˆ˜
                    title = metadata.get("title", "").lower()
                    content = doc.lower()
                    keyword_bonus = 0
                    
                    for keyword in keywords:
                        if keyword.lower() in title:
                            keyword_bonus += 0.2
                        elif keyword.lower() in content:
                            keyword_bonus += 0.1
                    
                    final_score = min(relevance_score + keyword_bonus, 1.0)
                    
                    chroma_results.append({
                        "content": doc,
                        "title": metadata.get("title", "ì œëª© ì—†ìŒ"),
                        "url": metadata.get("url", ""),
                        "published_date": metadata.get("published_date", ""),
                        "relevance_score": final_score,
                        "semantic_score": relevance_score,
                        "keyword_bonus": keyword_bonus,
                        "chunk_id": metadata.get("chunk_id", i),
                        "quality_tier": "high" if final_score > 0.7 else "medium" if final_score > 0.5 else "low"
                    })
        
        # í’ˆì§ˆë³„ ì •ë ¬ ë° ìƒìœ„ 5ê°œ ì„ íƒ
        chroma_results.sort(key=lambda x: x["relevance_score"], reverse=True)
        filtered_results = chroma_results[:5]
        
        chroma_process["steps"].append({
            "step": "quality_filtering",
            "filtered_count": len(filtered_results),
            "quality_distribution": {
                "high": len([r for r in filtered_results if r["quality_tier"] == "high"]),
                "medium": len([r for r in filtered_results if r["quality_tier"] == "medium"]),
                "low": len([r for r in filtered_results if r["quality_tier"] == "low"])
            }
        })
        
        print(f"    âœ… í’ˆì§ˆ í•„í„°ë§ ì™„ë£Œ:")
        print(f"      ğŸ“ˆ ê³ í’ˆì§ˆ: {len([r for r in filtered_results if r['quality_tier'] == 'high'])}ê°œ")
        print(f"      ğŸ“Š ì¤‘í’ˆì§ˆ: {len([r for r in filtered_results if r['quality_tier'] == 'medium'])}ê°œ")
        print(f"      ğŸ“‰ ì €í’ˆì§ˆ: {len([r for r in filtered_results if r['quality_tier'] == 'low'])}ê°œ")
        
        # Step 4: ê²°ê³¼ ìƒì„¸ í‘œì‹œ
        print(f"  ğŸ“„ Step 2.4: ìµœì¢… ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ({len(filtered_results)}ê°œ)")
        for i, result in enumerate(filtered_results, 1):
            print(f"    {i}. [{result['quality_tier'].upper()}] {result['title'][:60]}...")
            print(f"       ê´€ë ¨ë„: {result['relevance_score']:.3f} (ì˜ë¯¸: {result['semantic_score']:.3f} + í‚¤ì›Œë“œ: {result['keyword_bonus']:.3f})")
            print(f"       ë‚ ì§œ: {result['published_date']}")
        
        chroma_process["end_time"] = time.time()
        chroma_process["total_time"] = chroma_process["end_time"] - chroma_process["start_time"]
        chroma_process["success"] = True
        
        agent.log_step("chroma_search", chroma_process)
        
        return {
            "chroma_results": filtered_results,
            "chroma_process": chroma_process
        }
        
    except Exception as e:
        print(f"  âŒ ChromaDB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        print(f"  ğŸ”„ í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ê²€ìƒ‰ ì‹œë„...")
        
        # í´ë°±: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        try:
            news_collection = agent._get_chroma_client()
            all_results = news_collection.get(include=["documents", "metadatas"])
            
            keyword_results = []
            for i, (doc, metadata) in enumerate(zip(all_results["documents"], all_results["metadatas"])):
                title = metadata.get("title", "").lower()
                content = doc.lower()
                
                score = 0
                for keyword in keywords:
                    if keyword.lower() in title:
                        score += 3
                    elif keyword.lower() in content:
                        score += 1
                
                if score > 0:
                    keyword_results.append({
                        "content": doc,
                        "title": metadata.get("title", "ì œëª© ì—†ìŒ"),
                        "url": metadata.get("url", ""),
                        "published_date": metadata.get("published_date", ""),
                        "relevance_score": score / len(keywords),
                        "semantic_score": 0,
                        "keyword_bonus": score / len(keywords),
                        "chunk_id": metadata.get("chunk_id", i),
                        "quality_tier": "medium"
                    })
            
            keyword_results.sort(key=lambda x: x["relevance_score"], reverse=True)
            keyword_results = keyword_results[:5]
            
            chroma_process["search_method"] = "keyword_fallback"
            chroma_process["success"] = True
            chroma_process["error"] = str(e)
            
            print(f"    âœ… í´ë°± ê²€ìƒ‰ ì™„ë£Œ: {len(keyword_results)}ê°œ ë°œê²¬")
            
            return {
                "chroma_results": keyword_results,
                "chroma_process": chroma_process
            }
            
        except Exception as fallback_error:
            print(f"  âŒ í´ë°± ê²€ìƒ‰ë„ ì‹¤íŒ¨: {fallback_error}")
            chroma_process["success"] = False
            chroma_process["error"] = f"ì›ë³¸: {e}, í´ë°±: {fallback_error}"
            
            return {
                "chroma_results": [],
                "chroma_process": chroma_process
            }

def enhanced_neo4j_search_node(state: DetailedHybridRAGState) -> DetailedHybridRAGState:
    """ê°•í™”ëœ Neo4j ê²€ìƒ‰ (ì •êµí•œ í•„í„°ë§ ë° ê´€ë ¨ë„ í‰ê°€)"""
    agent = EnhancedHybridRAGAgent()
    question = state["question"]
    strategy = state.get("search_strategy", {})
    keywords = strategy.get("keywords", question.split())
    
    print(f"\nğŸ”— Step 3: Neo4j ê·¸ë˜í”„ ê²€ìƒ‰ (ì •êµí•œ ê´€ë ¨ë„ í‰ê°€)")
    print(f"ğŸ”‘ ì›ë³¸ í‚¤ì›Œë“œ: {', '.join(keywords)}")
    
    # Step 0: í‚¤ì›Œë“œ ì •ì œ ë° ì¡°í•© ìƒì„±
    print(f"  ğŸ§¹ Step 3.0: í‚¤ì›Œë“œ ì •ì œ ë° ê´€ë ¨ë„ ê°€ì¤‘ì¹˜ ì„¤ì •...")
    
    # í‚¤ì›Œë“œ ì •ì œ (ë¶ˆìš©ì–´ ì œê±° ë° ì¤‘ìš”ë„ ë¶„ë¥˜)
    primary_keywords = []  # í•µì‹¬ í‚¤ì›Œë“œ (ì˜ˆ: ì½”ìŠ¤í”¼, ì‚¼ì„±ì „ì)
    context_keywords = []  # ë§¥ë½ í‚¤ì›Œë“œ (ì˜ˆ: ìƒìŠ¹, ì›ì¸)
    
    # ê¸ˆìœµ/ê²½ì œ ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œ ì‹ë³„
    financial_entities = ["ì½”ìŠ¤í”¼", "kospi", "ì‚¼ì„±ì „ì", "skí•˜ì´ë‹‰ìŠ¤", "lg", "í˜„ëŒ€", "ì£¼ê°€", "ì¦ì‹œ", "ì§€ìˆ˜"]
    financial_concepts = ["ìƒìŠ¹", "í•˜ë½", "ê¸‰ë“±", "ê¸‰ë½", "ì›ì¸", "ì´ìœ ", "ì˜í–¥", "ìš”ì¸"]
    
    for keyword in keywords:
        keyword_lower = keyword.lower()
        if any(entity in keyword_lower for entity in financial_entities):
            primary_keywords.append(keyword)
        elif any(concept in keyword_lower for concept in financial_concepts):
            context_keywords.append(keyword)
        elif len(keyword) > 1:  # ê¸°íƒ€ í‚¤ì›Œë“œ
            context_keywords.append(keyword)
    
    # ìµœì†Œ í•˜ë‚˜ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ í•„ìš”
    if not primary_keywords and any(kw in question.lower() for kw in ["ì½”ìŠ¤í”¼", "ì£¼ê°€", "ì¦ì‹œ"]):
        primary_keywords = ["ì½”ìŠ¤í”¼"]
    
    print(f"    ğŸ¯ í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(primary_keywords) if primary_keywords else 'ì—†ìŒ'}")
    print(f"    ğŸ“ ë§¥ë½ í‚¤ì›Œë“œ: {', '.join(context_keywords)}")
    
    neo4j_process = {
        "start_time": time.time(),
        "keywords_used": keywords,
        "primary_keywords": primary_keywords,
        "context_keywords": context_keywords,
        "search_patterns": [],
        "steps": []
    }
    
    if not agent.neo4j_driver:
        print(f"  âŒ Neo4j ì—°ê²° ì—†ìŒ")
        neo4j_process["success"] = False
        neo4j_process["error"] = "Neo4j driver not available"
        return {"neo4j_results": [], "neo4j_process": neo4j_process}
    
    try:
        # Step 1: ì •êµí•œ ê²€ìƒ‰ íŒ¨í„´ ì„ íƒ
        print(f"  ğŸ§  Step 3.1: ì •êµí•œ ê²€ìƒ‰ íŒ¨í„´ ì„ íƒ...")
        
        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ íŒ¨í„´ ìš°ì„ ìˆœìœ„ ê²°ì •
        question_lower = question.lower()
        
        if any(word in question_lower for word in ["ì›ì¸", "ì´ìœ ", "ì™œ"]):
            pattern_priority = ["targeted_causal", "targeted_relationship", "contextual_network"]
            print(f"    ğŸ“Š ì •êµí•œ ì¸ê³¼ê´€ê³„ ê²€ìƒ‰ íŒ¨í„´ ì„ íƒ")
        elif any(word in question_lower for word in ["ê´€ë ¨", "ì—°ê´€", "ì˜í–¥"]):
            pattern_priority = ["contextual_network", "targeted_relationship", "targeted_causal"]
            print(f"    ğŸ•¸ï¸ ì •êµí•œ ë„¤íŠ¸ì›Œí¬ ê²€ìƒ‰ íŒ¨í„´ ì„ íƒ")
        else:
            pattern_priority = ["targeted_relationship", "targeted_causal", "contextual_network"]
            print(f"    ğŸ”— ì •êµí•œ ê´€ê³„ ê²€ìƒ‰ íŒ¨í„´ ì„ íƒ")
        
        # ì •êµí•œ ê²€ìƒ‰ íŒ¨í„´ ì •ì˜
        search_patterns = {
            "targeted_relationship": {
                "name": "í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬ ê´€ê³„ ê²€ìƒ‰",
                "query": """
                MATCH (a)-[r]->(b)
                WHERE (
                    any(pk IN $primary_keywords WHERE a.name CONTAINS pk OR b.name CONTAINS pk)
                    AND 
                    any(ck IN $context_keywords WHERE a.name CONTAINS ck OR b.name CONTAINS ck)
                )
                RETURN a.name as source, type(r) as relationship, b.name as target,
                       'targeted' as pattern_type
                LIMIT 8
                """,
                "description": "í•µì‹¬ í‚¤ì›Œë“œì™€ ë§¥ë½ í‚¤ì›Œë“œê°€ ëª¨ë‘ í¬í•¨ëœ ê´€ê³„ íƒìƒ‰"
            },
            "targeted_causal": {
                "name": "í•µì‹¬ ì¸ê³¼ê´€ê³„ ì²´ì¸",
                "query": """
                MATCH (a)-[:ì›ì¸ì´ë‹¤]->(b)-[:ê²°ê³¼ì´ë‹¤]->(c)
                WHERE (
                    any(pk IN $primary_keywords WHERE a.name CONTAINS pk OR b.name CONTAINS pk OR c.name CONTAINS pk)
                )
                RETURN a.name as cause, b.name as intermediate, c.name as effect,
                       'targeted_causal' as pattern_type
                LIMIT 5
                """,
                "description": "í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì›ì¸â†’ê²°ê³¼ ì²´ì¸ íƒìƒ‰"
            },
            "contextual_network": {
                "name": "ë§¥ë½ ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬",
                "query": """
                MATCH (center)-[r]-(connected)
                WHERE (
                    any(pk IN $primary_keywords WHERE center.name CONTAINS pk)
                    AND 
                    any(ck IN $context_keywords WHERE connected.name CONTAINS ck OR type(r) CONTAINS 'ìƒìŠ¹' OR type(r) CONTAINS 'ì›ì¸')
                )
                RETURN center.name as center_entity, type(r) as relationship, connected.name as related_entity,
                       'contextual' as pattern_type
                LIMIT 6
                """,
                "description": "í•µì‹¬ ì—”í‹°í‹°ì™€ ë§¥ë½ì ìœ¼ë¡œ ê´€ë ¨ëœ ë„¤íŠ¸ì›Œí¬ íƒìƒ‰"
            }
        }
        
        # í•µì‹¬ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ í´ë°± íŒ¨í„´ ì‚¬ìš©
        if not primary_keywords:
            print(f"    âš ï¸ í•µì‹¬ í‚¤ì›Œë“œ ì—†ìŒ - í´ë°± ê²€ìƒ‰ íŒ¨í„´ ì‚¬ìš©")
            search_patterns["fallback"] = {
                "name": "í´ë°± í‚¤ì›Œë“œ ê²€ìƒ‰",
                "query": """
                MATCH (a)-[r]->(b)
                WHERE any(keyword IN $context_keywords WHERE a.name CONTAINS keyword OR b.name CONTAINS keyword)
                RETURN a.name as source, type(r) as relationship, b.name as target,
                       'fallback' as pattern_type
                LIMIT 5
                """,
                "description": "ë§¥ë½ í‚¤ì›Œë“œë§Œì„ ì‚¬ìš©í•œ ê¸°ë³¸ ê²€ìƒ‰"
            }
            pattern_priority = ["fallback"]
        
        neo4j_results = []
        
        # Step 2: ì„ íƒëœ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ íŒ¨í„´ ì‹¤í–‰
        print(f"  ğŸ” Step 3.2: ì •êµí•œ ê·¸ë˜í”„ ê²€ìƒ‰ ì‹¤í–‰...")
        
        with agent.neo4j_driver.session() as session:
            for i, pattern_key in enumerate(pattern_priority, 1):
                if pattern_key not in search_patterns:
                    continue
                    
                pattern = search_patterns[pattern_key]
                print(f"    ğŸ¯ íŒ¨í„´ {i}: {pattern['name']} ì‹¤í–‰ ì¤‘...")
                print(f"       ğŸ“ ì„¤ëª…: {pattern['description']}")
                
                try:
                    start_pattern = time.time()
                    
                    # íŒ¨í„´ì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ì„¤ì •
                    if pattern_key == "fallback":
                        query_params = {"context_keywords": context_keywords}
                    else:
                        query_params = {
                            "primary_keywords": primary_keywords,
                            "context_keywords": context_keywords
                        }
                    
                    results = session.run(pattern["query"], query_params).data()
                    pattern_time = time.time() - start_pattern
                    
                    if results:
                        print(f"       âœ… ì„±ê³µ: {len(results)}ê°œ ê´€ê³„ ë°œê²¬ (ì†Œìš”ì‹œê°„: {pattern_time:.2f}ì´ˆ)")
                        
                        # ê²°ê³¼ì— íŒ¨í„´ ì •ë³´ì™€ ê´€ë ¨ë„ ì ìˆ˜ ì¶”ê°€
                        for result in results:
                            result["search_pattern"] = pattern_key
                            result["pattern_name"] = pattern["name"]
                            
                            # ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚° (ì•ˆì „í•œ ì²˜ë¦¬)
                            try:
                                relevance_score = calculate_graph_relevance(result, primary_keywords, context_keywords, question)
                                result["relevance_score"] = relevance_score
                            except Exception as relevance_error:
                                print(f"         âš ï¸ ê´€ë ¨ë„ ê³„ì‚° ì˜¤ë¥˜: {relevance_error}")
                                print(f"         ğŸ“‹ ê²°ê³¼ êµ¬ì¡°: {result}")
                                result["relevance_score"] = 0.5  # ê¸°ë³¸ê°’
                        
                        # ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                        results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
                        
                        neo4j_results.extend(results)
                        
                        neo4j_process["search_patterns"].append({
                            "pattern": pattern_key,
                            "name": pattern["name"],
                            "results_count": len(results),
                            "avg_relevance": sum(r.get("relevance_score", 0) for r in results) / len(results) if results else 0,
                            "time_taken": pattern_time,
                            "success": True
                        })
                        
                    else:
                        print(f"       âš ï¸ ê²°ê³¼ ì—†ìŒ (ì†Œìš”ì‹œê°„: {pattern_time:.2f}ì´ˆ)")
                        neo4j_process["search_patterns"].append({
                            "pattern": pattern_key,
                            "name": pattern["name"],
                            "results_count": 0,
                            "time_taken": pattern_time,
                            "success": True
                        })
                        
                except Exception as pattern_error:
                    print(f"       âŒ íŒ¨í„´ ì‹¤í–‰ ì˜¤ë¥˜: {pattern_error}")
                    neo4j_process["search_patterns"].append({
                        "pattern": pattern_key,
                        "name": pattern["name"],
                        "error": str(pattern_error),
                        "success": False
                    })
        
        # Step 3: ê´€ë ¨ë„ ê¸°ë°˜ í•„í„°ë§
        print(f"  ğŸ“Š Step 3.3: ê´€ë ¨ë„ ê¸°ë°˜ ê²°ê³¼ í•„í„°ë§...")
        
        if neo4j_results:
            # ê´€ë ¨ë„ ì ìˆ˜ê°€ ì—†ëŠ” ê²°ê³¼ì— ê¸°ë³¸ê°’ ì„¤ì •
            for result in neo4j_results:
                if "relevance_score" not in result:
                    result["relevance_score"] = 0.0
            # ê´€ë ¨ë„ ì„ê³„ê°’ ì ìš© (0.3 ì´ìƒë§Œ ìœ ì§€)
            filtered_results = [r for r in neo4j_results if r.get("relevance_score", 0) >= 0.3]
            
            # ì¤‘ë³µ ì œê±° (ë™ì¼í•œ ê´€ê³„)
            unique_results = []
            seen_relationships = set()
            
            for result in filtered_results:
                try:
                    if "relationship" in result:
                        key = f"{result['source']}-{result['relationship']}-{result['target']}"
                    elif "cause" in result:
                        key = f"{result['cause']}-{result['intermediate']}-{result['effect']}"
                    else:
                        key = str(result)
                    
                    if key not in seen_relationships:
                        seen_relationships.add(key)
                        unique_results.append(result)
                except Exception as key_error:
                    print(f"    âš ï¸ ì¤‘ë³µ ì œê±° ì¤‘ ì˜¤ë¥˜: {key_error}")
                    print(f"    ğŸ“‹ ë¬¸ì œ ê²°ê³¼: {result}")
                    # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ê²°ê³¼ëŠ” í¬í•¨ (ë‹¨, ê¸°ë³¸ í‚¤ ì‚¬ìš©)
                    key = str(hash(str(result)))
                    if key not in seen_relationships:
                        seen_relationships.add(key)
                        unique_results.append(result)
            
            neo4j_results = unique_results[:10]  # ìƒìœ„ 10ê°œë¡œ ì œí•œ
            
            print(f"    âœ… í•„í„°ë§ ì™„ë£Œ: {len(filtered_results)}ê°œ â†’ {len(neo4j_results)}ê°œ (ì¤‘ë³µ ì œê±°)")
            
            # ê´€ë ¨ë„ë³„ ë¶„í¬
            high_rel = len([r for r in neo4j_results if r.get("relevance_score", 0) >= 0.7])
            med_rel = len([r for r in neo4j_results if 0.5 <= r.get("relevance_score", 0) < 0.7])
            low_rel = len([r for r in neo4j_results if 0.3 <= r.get("relevance_score", 0) < 0.5])
            
            print(f"    ğŸ“Š ê´€ë ¨ë„ ë¶„í¬: ê³ ê´€ë ¨({high_rel}ê°œ) ì¤‘ê´€ë ¨({med_rel}ê°œ) ì €ê´€ë ¨({low_rel}ê°œ)")
            
            # ê´€ê³„ ìœ í˜•ë³„ ë¶„ë¥˜
            relationship_types = {}
            
            for result in neo4j_results:
                try:
                    if "relationship" in result:
                        rel_type = result["relationship"]
                        relationship_types[rel_type] = relationship_types.get(rel_type, 0) + 1
                except Exception as rel_error:
                    print(f"    âš ï¸ ê´€ê³„ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜: {rel_error}")
                    print(f"    ğŸ“‹ ë¬¸ì œ ê²°ê³¼: {result}")
            
            print(f"    ğŸ“ˆ ê´€ê³„ ìœ í˜• ë¶„í¬:")
            for rel_type, count in sorted(relationship_types.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"       - {rel_type}: {count}ê°œ")
            
            neo4j_process["analysis"] = {
                "relationship_types": relationship_types,
                "relevance_distribution": {"high": high_rel, "medium": med_rel, "low": low_rel},
                "total_relationships": len(neo4j_results)
            }
        else:
            print(f"    âš ï¸ Neo4j ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
        
        # Step 4: ì •ì œëœ ê²°ê³¼ ìƒì„¸ í‘œì‹œ
        print(f"  ğŸ¯ Step 3.4: ì •ì œëœ ê·¸ë˜í”„ ê²€ìƒ‰ ê²°ê³¼ ({len(neo4j_results)}ê°œ)")
        
        for i, result in enumerate(neo4j_results, 1):
            try:
                relevance = result.get("relevance_score", 0)
                pattern = result.get("search_pattern", "unknown").upper()
                
                if "relationship" in result:
                    print(f"    {i}. [{pattern}] {result['source']} -[{result['relationship']}]-> {result['target']}")
                    print(f"       ê´€ë ¨ë„: {relevance:.3f}")
                elif "cause" in result:
                    print(f"    {i}. [CAUSAL] {result['cause']} â†’ {result['intermediate']} â†’ {result['effect']}")
                    print(f"       ê´€ë ¨ë„: {relevance:.3f}")
                elif "center_entity" in result:
                    print(f"    {i}. [NETWORK] {result['center_entity']} -[{result.get('relationship', 'related')}]-> {result['related_entity']}")
                    print(f"       ê´€ë ¨ë„: {relevance:.3f}")
                else:
                    print(f"    {i}. [UNKNOWN] {result}")
                    print(f"       ê´€ë ¨ë„: {relevance:.3f}")
            except Exception as display_error:
                print(f"    {i}. [ERROR] ê²°ê³¼ í‘œì‹œ ì˜¤ë¥˜: {display_error}")
                print(f"       ì›ë³¸ ë°ì´í„°: {result}")
        
        neo4j_process["end_time"] = time.time()
        neo4j_process["total_time"] = neo4j_process["end_time"] - neo4j_process["start_time"]
        neo4j_process["success"] = True
        neo4j_process["total_results"] = len(neo4j_results)
        
        agent.log_step("neo4j_search", neo4j_process)
        
        return {
            "neo4j_results": neo4j_results,
            "neo4j_process": neo4j_process
        }
        
    except Exception as e:
        print(f"  âŒ Neo4j ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        neo4j_process["success"] = False
        neo4j_process["error"] = str(e)
        neo4j_process["end_time"] = time.time()
        
        return {
            "neo4j_results": [],
            "neo4j_process": neo4j_process
        }
    finally:
        if agent.neo4j_driver:
            agent.neo4j_driver.close()

def calculate_graph_relevance(result: Dict, primary_keywords: List[str], context_keywords: List[str], question: str) -> float:
    """ê·¸ë˜í”„ ê²€ìƒ‰ ê²°ê³¼ì˜ ì§ˆë¬¸ ê´€ë ¨ë„ ê³„ì‚° (ì•ˆì „í•œ ì²˜ë¦¬)"""
    try:
        score = 0.0
        question_lower = question.lower()
        
        # ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì•ˆì „í•œ ë°©ì‹)
        text_fields = []
        
        # ë‹¤ì–‘í•œ ê²°ê³¼ êµ¬ì¡° ì²˜ë¦¬
        if "source" in result and "target" in result:
            # ì¼ë°˜ ê´€ê³„: source -[relationship]-> target
            text_fields = [
                str(result.get("source", "")), 
                str(result.get("target", "")), 
                str(result.get("relationship", ""))
            ]
        elif "cause" in result and "intermediate" in result and "effect" in result:
            # ì¸ê³¼ê´€ê³„ ì²´ì¸: cause -> intermediate -> effect
            text_fields = [
                str(result.get("cause", "")),
                str(result.get("intermediate", "")),
                str(result.get("effect", ""))
            ]
        elif "center_entity" in result and "related_entity" in result:
            # ë„¤íŠ¸ì›Œí¬: center_entity -[relationship]-> related_entity
            text_fields = [
                str(result.get("center_entity", "")),
                str(result.get("related_entity", "")),
                str(result.get("relationship", ""))
            ]
        else:
            # ê¸°íƒ€ êµ¬ì¡°: ëª¨ë“  ê°’ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            text_fields = [str(v) for v in result.values() if v is not None]
        
        # ë¹ˆ í•„ë“œ ì œê±°
        text_fields = [field for field in text_fields if field and field != "None"]
        
        if not text_fields:
            return 0.0
        
        full_text = " ".join(text_fields).lower()
        
        # 1. í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­ (ë†’ì€ ê°€ì¤‘ì¹˜)
        for pk in primary_keywords:
            if pk and pk.lower() in full_text:
                score += 0.4
        
        # 2. ë§¥ë½ í‚¤ì›Œë“œ ë§¤ì¹­ (ì¤‘ê°„ ê°€ì¤‘ì¹˜)
        for ck in context_keywords:
            if ck and ck.lower() in full_text:
                score += 0.2
        
        # 3. ì§ˆë¬¸ ë„ë©”ì¸ ì¼ì¹˜ì„± (ê¸ˆìœµ/ê²½ì œ ê´€ë ¨)
        financial_terms = ["ì£¼ê°€", "ì½”ìŠ¤í”¼", "ì¦ì‹œ", "ìƒìŠ¹", "í•˜ë½", "ê²½ì œ", "ê¸ˆìœµ", "íˆ¬ì", "ì‹œì¥"]
        domain_match = sum(1 for term in financial_terms if term in full_text)
        score += min(domain_match * 0.1, 0.3)
        
        # 4. ê´€ê³„ì˜ ì˜ë¯¸ì  ì í•©ì„±
        if "relationship" in result:
            rel = str(result.get("relationship", ""))
            if any(word in rel for word in ["ì›ì¸", "ê²°ê³¼", "ì˜í–¥", "ìƒìŠ¹", "ì¦ê°€"]):
                score += 0.2
        
        # 5. íŒ¨ë„í‹°: ì§ˆë¬¸ê³¼ ë¬´ê´€í•œ ì£¼ì œ
        irrelevant_terms = ["ì—ì´ë¯¸", "íìŠ¤", "ê°€ìŠ´", "ì¶•ì†Œìˆ ", "ë°•ë³´ì˜", "ì—°ê¸°ì‡¼"]
        if any(term in full_text for term in irrelevant_terms):
            score -= 0.5
        
        return max(0.0, min(1.0, score))  # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
        
    except Exception as e:
        # ëª¨ë“  ì˜ˆì™¸ ìƒí™©ì—ì„œ ê¸°ë³¸ê°’ ë°˜í™˜
        print(f"ê´€ë ¨ë„ ê³„ì‚° ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return 0.5

def quality_assessment_node(state: DetailedHybridRAGState) -> DetailedHybridRAGState:
    """ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ì¢…í•© í‰ê°€"""
    question = state["question"]
    chroma_results = state.get("chroma_results", [])
    neo4j_results = state.get("neo4j_results", [])
    chroma_process = state.get("chroma_process", {})
    neo4j_process = state.get("neo4j_process", {})
    
    print(f"\nğŸ“Š Step 4: ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ì¢…í•© í‰ê°€")
    
    quality_assessment = {
        "chroma_quality": {},
        "neo4j_quality": {},
        "overall_quality": {},
        "recommendations": []
    }
    
    # ChromaDB ê²°ê³¼ í’ˆì§ˆ í‰ê°€
    print(f"  ğŸ“„ ChromaDB ê²°ê³¼ í’ˆì§ˆ ë¶„ì„:")
    
    if chroma_results:
        avg_relevance = sum(r["relevance_score"] for r in chroma_results) / len(chroma_results)
        high_quality_count = len([r for r in chroma_results if r["quality_tier"] == "high"])
        has_recent_news = any(r.get("published_date", "").startswith("2025-07") for r in chroma_results)
        
        quality_assessment["chroma_quality"] = {
            "result_count": len(chroma_results),
            "avg_relevance": avg_relevance,
            "high_quality_ratio": high_quality_count / len(chroma_results),
            "has_recent_content": has_recent_news,
            "search_success": chroma_process.get("success", False)
        }
        
        print(f"    âœ… ê²°ê³¼ ìˆ˜: {len(chroma_results)}ê°œ")
        print(f"    ğŸ“Š í‰ê·  ê´€ë ¨ë„: {avg_relevance:.3f}")
        print(f"    ğŸŒŸ ê³ í’ˆì§ˆ ë¹„ìœ¨: {high_quality_count}/{len(chroma_results)} ({high_quality_count/len(chroma_results)*100:.1f}%)")
        print(f"    ğŸ“… ìµœì‹  ë‰´ìŠ¤: {'ìˆìŒ' if has_recent_news else 'ì—†ìŒ'}")
        
        if avg_relevance < 0.5:
            quality_assessment["recommendations"].append("ChromaDB ê²€ìƒ‰ í‚¤ì›Œë“œ ë˜ëŠ” ì „ëµ ê°œì„  í•„ìš”")
            
    else:
        quality_assessment["chroma_quality"] = {
            "result_count": 0,
            "avg_relevance": 0,
            "high_quality_ratio": 0,
            "has_recent_content": False,
            "search_success": False
        }
        print(f"    âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        quality_assessment["recommendations"].append("ChromaDB ê²€ìƒ‰ ì „ëµ ì „ë©´ ì¬ê²€í†  í•„ìš”")
    
    # Neo4j ê²°ê³¼ í’ˆì§ˆ í‰ê°€
    print(f"  ğŸ”— Neo4j ê²°ê³¼ í’ˆì§ˆ ë¶„ì„:")
    
    if neo4j_results:
        successful_patterns = len([p for p in neo4j_process.get("search_patterns", []) if p.get("success", False)])
        relationship_diversity = len(set(r.get("relationship", "unknown") for r in neo4j_results if "relationship" in r))
        entity_diversity = len(set(str(r.get("source_type", "")) + str(r.get("target_type", "")) for r in neo4j_results))
        
        quality_assessment["neo4j_quality"] = {
            "result_count": len(neo4j_results),
            "successful_patterns": successful_patterns,
            "relationship_diversity": relationship_diversity,
            "entity_diversity": entity_diversity,
            "search_success": neo4j_process.get("success", False)
        }
        
        print(f"    âœ… ê´€ê³„ ìˆ˜: {len(neo4j_results)}ê°œ")
        print(f"    ğŸ¯ ì„±ê³µ íŒ¨í„´: {successful_patterns}ê°œ")
        print(f"    ğŸ”„ ê´€ê³„ ë‹¤ì–‘ì„±: {relationship_diversity}ê°œ ìœ í˜•")
        print(f"    ğŸ·ï¸ ì—”í‹°í‹° ë‹¤ì–‘ì„±: {entity_diversity}ê°œ ì¡°í•©")
        
        if len(neo4j_results) < 3:
            quality_assessment["recommendations"].append("Neo4j ê²€ìƒ‰ ë²”ìœ„ í™•ì¥ ë˜ëŠ” í‚¤ì›Œë“œ ì¡°ì • í•„ìš”")
            
    else:
        quality_assessment["neo4j_quality"] = {
            "result_count": 0,
            "successful_patterns": 0,
            "relationship_diversity": 0,
            "entity_diversity": 0,
            "search_success": False
        }
        print(f"    âŒ ê·¸ë˜í”„ ê´€ê³„ ì—†ìŒ")
        quality_assessment["recommendations"].append("Neo4j ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸ ë˜ëŠ” ê²€ìƒ‰ ì „ëµ ë³€ê²½ í•„ìš”")
    
    # ì „ì²´ í’ˆì§ˆ ì¢…í•© í‰ê°€
    print(f"  ğŸ¯ ì¢…í•© í’ˆì§ˆ í‰ê°€:")
    
    chroma_score = min(quality_assessment["chroma_quality"]["avg_relevance"] * 0.7 + 
                      quality_assessment["chroma_quality"]["high_quality_ratio"] * 0.3, 1.0)
    
    neo4j_score = 0.0
    if neo4j_results and len(neo4j_results) > 0:
        neo4j_score += 0.4
        neo4j_score += min(quality_assessment["neo4j_quality"]["relationship_diversity"] / 10 * 0.3, 0.3)
        neo4j_score += min(quality_assessment["neo4j_quality"]["successful_patterns"] / 3 * 0.3, 0.3)
    neo4j_score = min(neo4j_score, 1.0)
    
    # ë°ì´í„° ìƒí˜¸ ë³´ì™„ì„±
    complementarity = 0.1 if chroma_results and neo4j_results else 0
    
    overall_score = (chroma_score * 0.6 + neo4j_score * 0.3 + complementarity * 0.1)
    
    quality_assessment["overall_quality"] = {
        "chroma_score": chroma_score,
        "neo4j_score": neo4j_score,
        "complementarity": complementarity,
        "overall_score": overall_score,
        "quality_tier": "high" if overall_score > 0.7 else "medium" if overall_score > 0.4 else "low"
    }
    
    print(f"    ğŸ“Š ChromaDB ì ìˆ˜: {chroma_score:.3f}")
    print(f"    ğŸ”— Neo4j ì ìˆ˜: {neo4j_score:.3f}")
    print(f"    ğŸ¤ ìƒí˜¸ë³´ì™„ì„±: {complementarity:.3f}")
    print(f"    ğŸ¯ ì¢…í•© ì ìˆ˜: {overall_score:.3f} ({quality_assessment['overall_quality']['quality_tier'].upper()})")
    
    if quality_assessment["recommendations"]:
        print(f"  ğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(quality_assessment["recommendations"], 1):
            print(f"    {i}. {rec}")
    
    return {"quality_assessment": quality_assessment}

def enhanced_synthesis_node(state: DetailedHybridRAGState) -> DetailedHybridRAGState:
    """ê°•í™”ëœ í†µí•© ë‹µë³€ ìƒì„± (ìƒì„¸ ê³¼ì • ì¶”ì )"""
    agent = EnhancedHybridRAGAgent()
    question = state["question"]
    chroma_results = state.get("chroma_results", [])
    neo4j_results = state.get("neo4j_results", [])
    quality_assessment = state.get("quality_assessment", {})
    
    print(f"\nğŸ§  Step 5: í†µí•© ë‹µë³€ ìƒì„± (ìƒì„¸ ê³¼ì •)")
    
    synthesis_process = {
        "start_time": time.time(),
        "steps": [],
        "data_integration": {},
        "answer_strategy": {}
    }
    
    # Step 1: ë°ì´í„° í†µí•© ì „ëµ ê²°ì •
    print(f"  ğŸ¯ Step 5.1: ë°ì´í„° í†µí•© ì „ëµ ê²°ì •...")
    
    overall_quality = quality_assessment.get("overall_quality", {})
    chroma_quality = quality_assessment.get("chroma_quality", {})
    neo4j_quality = quality_assessment.get("neo4j_quality", {})
    
    # í†µí•© ì „ëµ ê²°ì •
    if chroma_results and neo4j_results:
        integration_strategy = "hybrid_synthesis"
        print(f"    ğŸ¤ í•˜ì´ë¸Œë¦¬ë“œ í†µí•©: ë‰´ìŠ¤ + ê·¸ë˜í”„ ê²°í•©")
    elif chroma_results:
        integration_strategy = "news_focused"
        print(f"    ğŸ“° ë‰´ìŠ¤ ì¤‘ì‹¬: ChromaDB ê²°ê³¼ ê¸°ë°˜")
    elif neo4j_results:
        integration_strategy = "graph_focused"  
        print(f"    ğŸ”— ê·¸ë˜í”„ ì¤‘ì‹¬: Neo4j ê´€ê³„ ê¸°ë°˜")
    else:
        integration_strategy = "knowledge_fallback"
        print(f"    ğŸ§  ì§€ì‹ í´ë°±: ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ (ì£¼ì˜: í™˜ê° ìœ„í—˜)")
    
    synthesis_process["answer_strategy"]["integration_strategy"] = integration_strategy
    
    # Step 2: í•µì‹¬ ë°ì´í„° ì¶”ì¶œ ë° êµ¬ì¡°í™”
    print(f"  ğŸ“Š Step 5.2: í•µì‹¬ ë°ì´í„° ì¶”ì¶œ ë° êµ¬ì¡°í™”...")
    
    # ë‰´ìŠ¤ ë°ì´í„° êµ¬ì¡°í™”
    structured_news = []
    if chroma_results:
        for i, result in enumerate(chroma_results[:3], 1):
            structured_news.append({
                "rank": i,
                "title": result["title"],
                "content_snippet": result["content"][:200] + "..." if len(result["content"]) > 200 else result["content"],
                "date": result["published_date"],
                "url": result["url"],
                "relevance": result["relevance_score"],
                "quality_tier": result["quality_tier"]
            })
        
        print(f"    ğŸ“„ êµ¬ì¡°í™”ëœ ë‰´ìŠ¤: {len(structured_news)}ê°œ")
        for news in structured_news:
            print(f"       #{news['rank']} [{news['quality_tier'].upper()}] {news['title'][:50]}... (ê´€ë ¨ë„: {news['relevance']:.3f})")
    
    # ê·¸ë˜í”„ ê´€ê³„ êµ¬ì¡°í™”
    structured_relationships = []
    if neo4j_results:
        for i, result in enumerate(neo4j_results[:5], 1):
            try:
                if "source" in result and "target" in result:
                    # ì¼ë°˜ ê´€ê³„ êµ¬ì¡°
                    structured_relationships.append({
                        "rank": i,
                        "type": "direct_relationship",
                        "source": result["source"],
                        "relationship": result["relationship"],
                        "target": result["target"],
                        "pattern": result.get("search_pattern", "unknown"),
                        "relevance": result.get("relevance_score", 0)
                    })
                elif "cause" in result and "intermediate" in result and "effect" in result:
                    # ì¸ê³¼ê´€ê³„ ì²´ì¸ êµ¬ì¡°
                    structured_relationships.append({
                        "rank": i,
                        "type": "causal_chain",
                        "cause": result["cause"],
                        "intermediate": result["intermediate"],
                        "effect": result["effect"],
                        "pattern": "causal",
                        "relevance": result.get("relevance_score", 0)
                    })
                elif "center_entity" in result and "related_entity" in result:
                    # ë„¤íŠ¸ì›Œí¬ ê´€ê³„ êµ¬ì¡°
                    structured_relationships.append({
                        "rank": i,
                        "type": "network_relationship",
                        "source": result["center_entity"],  # center_entityë¥¼ sourceë¡œ ë§¤í•‘
                        "relationship": result["relationship"],
                        "target": result["related_entity"],  # related_entityë¥¼ targetìœ¼ë¡œ ë§¤í•‘
                        "pattern": result.get("search_pattern", "network"),
                        "relevance": result.get("relevance_score", 0)
                    })
                else:
                    # ê¸°íƒ€ êµ¬ì¡° - ê°€ëŠ¥í•œ í•œ ì •ë³´ ì¶”ì¶œ
                    structured_relationships.append({
                        "rank": i,
                        "type": "unknown",
                        "source": str(result.get("source", result.get("center_entity", "ì•Œìˆ˜ì—†ìŒ"))),
                        "relationship": str(result.get("relationship", "ê´€ë ¨")),
                        "target": str(result.get("target", result.get("related_entity", "ì•Œìˆ˜ì—†ìŒ"))),
                        "pattern": result.get("search_pattern", "unknown"),
                        "relevance": result.get("relevance_score", 0)
                    })
            except Exception as struct_error:
                print(f"    âš ï¸ ê´€ê³„ êµ¬ì¡°í™” ì˜¤ë¥˜: {struct_error}")
                print(f"    ğŸ“‹ ë¬¸ì œ ê²°ê³¼: {result}")
                # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ê¸°ë³¸ êµ¬ì¡°ë¡œ í¬í•¨
                structured_relationships.append({
                    "rank": i,
                    "type": "error",
                    "source": "ì˜¤ë¥˜",
                    "relationship": "ì•Œìˆ˜ì—†ìŒ",
                    "target": "ì˜¤ë¥˜",
                    "pattern": "error",
                    "relevance": 0
                })
        
        print(f"    ğŸ”— êµ¬ì¡°í™”ëœ ê´€ê³„: {len(structured_relationships)}ê°œ")
        for rel in structured_relationships:
            if rel["type"] == "direct_relationship":
                print(f"       #{rel['rank']} [{rel['pattern'].upper()}] {rel['source']} -[{rel['relationship']}]-> {rel['target']} (ê´€ë ¨ë„: {rel['relevance']:.3f})")
            elif rel["type"] == "causal_chain":
                print(f"       #{rel['rank']} [CAUSAL] {rel['cause']} â†’ {rel['intermediate']} â†’ {rel['effect']} (ê´€ë ¨ë„: {rel['relevance']:.3f})")
            elif rel["type"] == "network_relationship":
                print(f"       #{rel['rank']} [NETWORK] {rel['source']} -[{rel['relationship']}]-> {rel['target']} (ê´€ë ¨ë„: {rel['relevance']:.3f})")
            else:
                print(f"       #{rel['rank']} [{rel['type'].upper()}] {rel['source']} -[{rel['relationship']}]-> {rel['target']} (ê´€ë ¨ë„: {rel['relevance']:.3f})")
    
    synthesis_process["data_integration"] = {
        "news_count": len(structured_news),
        "relationship_count": len(structured_relationships),
        "integration_strategy": integration_strategy
    }
    
    # Step 3: LLM ê¸°ë°˜ í†µí•© ë‹µë³€ ìƒì„±
    print(f"  ğŸ¤– Step 5.3: LLM ê¸°ë°˜ í†µí•© ë‹µë³€ ìƒì„±...")
    
    # ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ (ì „ëµë³„)
    if integration_strategy == "hybrid_synthesis":
        synthesis_prompt = f"""
ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ì™€ ê´€ê³„ ê·¸ë˜í”„ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ì •í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

=== ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ ===
{chr(10).join([f"[ë‰´ìŠ¤ {news['rank']}] {news['title']} ({news['date']})\\në‚´ìš©: {news['content_snippet']}\\nì¶œì²˜: {news['url']}\\nê´€ë ¨ë„: {news['relevance']:.3f}\\n" for news in structured_news]) if structured_news else "ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ"}

=== ê´€ê³„ ê·¸ë˜í”„ ì •ë³´ ===
{chr(10).join([f"[ê´€ê³„ {rel['rank']}] {rel['source']} -[{rel['relationship']}]-> {rel['target']} (íŒ¨í„´: {rel['pattern']}, ê´€ë ¨ë„: {rel['relevance']:.3f})" if rel['type'] in ['direct_relationship', 'network_relationship', 'unknown', 'error'] else f"[ì¸ê³¼ {rel['rank']}] {rel['cause']} â†’ {rel['intermediate']} â†’ {rel['effect']} (ê´€ë ¨ë„: {rel['relevance']:.3f})" for rel in structured_relationships]) if structured_relationships else "ê´€ê³„ ì •ë³´ ì—†ìŒ"}

ë‹µë³€ ìƒì„± ì§€ì¹¨:
1. ë‰´ìŠ¤ ê¸°ì‚¬ì˜ êµ¬ì²´ì  ì‚¬ì‹¤ì„ ë‹µë³€ì˜ í•µì‹¬ìœ¼ë¡œ ì‚¬ìš©
2. ê·¸ë˜í”„ ê´€ê³„ë¡œ ë§¥ë½ê³¼ ë°°ê²½ ì„¤ëª… ë³´ê°•
3. ë‚ ì§œ, ì¶œì²˜, êµ¬ì²´ì  ìˆ˜ì¹˜ ë“± ì‚¬ì‹¤ ì •ë³´ ëª…ì‹œ
4. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ ì§€ì‹ ì‚¬ìš© ê¸ˆì§€
5. ìì—°ìŠ¤ëŸ½ê³  ì²´ê³„ì ì¸ ë¬¸ë‹¨ êµ¬ì„±
6. ì •ë³´ì˜ ì‹ ë¢°ë„ì™€ í•œê³„ ëª…ì‹œ

ë‹µë³€:
"""
    elif integration_strategy == "news_focused":
        synthesis_prompt = f"""
ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

=== ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ ===
{chr(10).join([f"[ë‰´ìŠ¤ {news['rank']}] {news['title']} ({news['date']})\\në‚´ìš©: {news['content_snippet']}\\nì¶œì²˜: {news['url']}\\n" for news in structured_news])}

ë‹µë³€ì€ ì˜¤ì§ ì œê³µëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
    elif integration_strategy == "graph_focused":
        synthesis_prompt = f"""
ê´€ê³„ ê·¸ë˜í”„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

=== ê´€ê³„ ì •ë³´ ===
{chr(10).join([f"[ê´€ê³„ {rel['rank']}] {rel['source']} -[{rel['relationship']}]-> {rel['target']}" if rel['type'] in ['direct_relationship', 'network_relationship', 'unknown', 'error'] else f"[ì¸ê³¼ {rel['rank']}] {rel['cause']} â†’ {rel['intermediate']} â†’ {rel['effect']}" for rel in structured_relationships])}

ë‹µë³€ì€ ì˜¤ì§ ì œê³µëœ ê´€ê³„ ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
    else:
        synthesis_prompt = f"""
ê²€ìƒ‰ëœ êµ¬ì²´ì  ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ ë¶ˆê°€ëŠ¥í•¨ì„ ëª…ì‹œí•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€: ì£„ì†¡í•©ë‹ˆë‹¤. '{question}'ì— ëŒ€í•œ êµ¬ì²´ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 
ë” ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ì„œëŠ” ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.
"""
    
    try:
        start_llm = time.time()
        final_answer = agent.llm.invoke(synthesis_prompt, config={"callbacks": [langfuse_handler]}).content
        llm_time = time.time() - start_llm
        
        print(f"    âœ… LLM ë‹µë³€ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {llm_time:.2f}ì´ˆ)")
        print(f"    ğŸ“ ë‹µë³€ ê¸¸ì´: {len(final_answer)}ì")
        
        # Step 4: ì‹ ë¢°ë„ ë° ë©”íƒ€ë°ì´í„° ê³„ì‚°
        print(f"  ğŸ“Š Step 5.4: ì‹ ë¢°ë„ ë° ë©”íƒ€ë°ì´í„° ê³„ì‚°...")
        
        # ì‹ ë¢°ë„ ê³„ì‚° (ì •êµí•œ ì•Œê³ ë¦¬ì¦˜)
        base_confidence = 0.3
        
        # ë‰´ìŠ¤ ê¸°ë°˜ ì‹ ë¢°ë„ ê°€ì‚°
        if chroma_results:
            news_confidence = min(chroma_quality.get("avg_relevance", 0) * 0.4, 0.4)
            quality_bonus = chroma_quality.get("high_quality_ratio", 0) * 0.2
            recency_bonus = 0.1 if chroma_quality.get("has_recent_content", False) else 0
            base_confidence += news_confidence + quality_bonus + recency_bonus
        
        # ê·¸ë˜í”„ ê¸°ë°˜ ì‹ ë¢°ë„ ê°€ì‚°
        if neo4j_results:
            graph_confidence = min(len(neo4j_results) / 10 * 0.2, 0.2)
            diversity_bonus = min(neo4j_quality.get("relationship_diversity", 0) / 5 * 0.1, 0.1)
            base_confidence += graph_confidence + diversity_bonus
        
        # í†µí•© ë³´ë„ˆìŠ¤
        if chroma_results and neo4j_results:
            base_confidence += 0.1
        
        final_confidence = min(base_confidence, 1.0)
        
        # ì¶œì²˜ ì •ë¦¬
        sources = []
        for news in structured_news:
            if news["url"]:
                sources.append(news["url"])
        
        print(f"    ğŸ¯ ìµœì¢… ì‹ ë¢°ë„: {final_confidence:.3f}")
        print(f"    ğŸ“š ì°¸ê³  ì¶œì²˜: {len(sources)}ê°œ")
        
        synthesis_process["end_time"] = time.time()
        synthesis_process["total_time"] = synthesis_process["end_time"] - synthesis_process["start_time"]
        synthesis_process["success"] = True
        synthesis_process["final_confidence"] = final_confidence
        synthesis_process["answer_length"] = len(final_answer)
        
        agent.log_step("synthesis", synthesis_process)
        
        return {
            "final_answer": final_answer,
            "confidence_score": final_confidence,
            "sources": sources,
            "synthesis_process": synthesis_process
        }
        
    except Exception as e:
        print(f"    âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
        
        synthesis_process["success"] = False
        synthesis_process["error"] = str(e)
        
        return {
            "final_answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. '{question}'ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
            "confidence_score": 0.0,
            "sources": [],
            "synthesis_process": synthesis_process
        }

def create_enhanced_hybrid_rag_graph():
    """ê°•í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ RAG ê·¸ë˜í”„ ìƒì„±"""
    checkpointer = InMemorySaver()
    
    graph = StateGraph(DetailedHybridRAGState)
    
    # ë…¸ë“œ ì¶”ê°€ (ìˆœì°¨ì  + ë³‘ë ¬ ì¡°í•©)
    graph.add_node("analyze_question", analyze_question_strategy)
    graph.add_node("chroma_search", enhanced_chroma_search_node)
    graph.add_node("neo4j_search", enhanced_neo4j_search_node)
    graph.add_node("quality_assessment", quality_assessment_node)
    graph.add_node("synthesis", enhanced_synthesis_node)
    
    # ì—£ì§€ ì •ì˜ (ê°œì„ ëœ í”Œë¡œìš°)
    graph.add_edge(START, "analyze_question")
    
    # ê²€ìƒ‰ ì „ëµì— ë”°ë¥¸ ë³‘ë ¬ ì‹¤í–‰
    graph.add_edge("analyze_question", "chroma_search")
    graph.add_edge("analyze_question", "neo4j_search")
    
    # í’ˆì§ˆ í‰ê°€ í›„ í†µí•©
    graph.add_edge("chroma_search", "quality_assessment")
    graph.add_edge("neo4j_search", "quality_assessment")
    graph.add_edge("quality_assessment", "synthesis")
    graph.add_edge("synthesis", END)
    
    return graph.compile(checkpointer=checkpointer)

def main():
    """ê°•í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ Enhanced Hybrid RAG Agent (ìƒì„¸ ê³¼ì • ì¶”ì )")
    print("=" * 60)
    
    # ê·¸ë˜í”„ ìƒì„±
    enhanced_graph = create_enhanced_hybrid_rag_graph()
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_questions = [
        # "ìµœê·¼ ì •ì¹˜ì  ì‚¬ê±´ë“¤ì´ êµ­ì • ìš´ì˜ì— ë¯¸ì¹œ ì˜í–¥ì€?",
        # "ì‚¼ì„±ì „ì ì´ì¬ìš© íšŒì¥ì˜ ë¬´ì£„ íŒê²°ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
        # "ìµœê·¼ í•œêµ­ ê²½ì œ ìƒí™©ì€ ì–´ë–¤ê°€ìš”?",
        "í˜„ì¬ í•œêµ­ ê²½ì œëŠ” ì–´ë–¤ìƒí™©ì¸ê°€ìš”? ì½”ìŠ¤í”¼ëŠ” ì™œ ìƒìŠ¹í•˜ë‚˜ìš”?",
        "ì½”ìŠ¤í”¼ ìƒìŠ¹ì— ë¯¸êµ­ê³¼ì˜ ê´€ê³„",
        ""
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*10} ì§ˆë¬¸ {i} {'='*10}")
        print(f"â“ {question}")
        print("=" * 60)
        
        # ì´ˆê¸° ìƒíƒœ
        initial_state = {
            "question": question,
            "search_strategy": {},
            "chroma_process": {},
            "neo4j_process": {},
            "quality_assessment": {},
            "synthesis_process": {},
            "chroma_results": [],
            "neo4j_results": [],
            "final_answer": "",
            "sources": [],
            "confidence_score": 0.0,
            "iteration_count": 0,
            "execution_log": []
        }
        
        # ì‹¤í–‰
        config = {"configurable": {"thread_id": f"enhanced_session_{i}"}, "callbacks": [langfuse_handler]}
        
        try:
            start_total = time.time()
            result = enhanced_graph.invoke(initial_state, config=config)
            total_time = time.time() - start_total
            
            print(f"\nğŸ‰ ì‹¤í–‰ ì™„ë£Œ (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
            print("=" * 60)
            
            print(f"\nğŸ“‹ ìµœì¢… ë‹µë³€:")
            print(result["final_answer"])
            
            print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
            print(f"  ğŸ¯ ì‹ ë¢°ë„: {result['confidence_score']:.3f}")
            print(f"  ğŸ“° ë‰´ìŠ¤ ì¶œì²˜: {len(result.get('chroma_results', []))}ê°œ")
            print(f"  ğŸ”— ê·¸ë˜í”„ ê´€ê³„: {len(result.get('neo4j_results', []))}ê°œ")
            print(f"  â±ï¸ ì´ ì²˜ë¦¬ì‹œê°„: {total_time:.2f}ì´ˆ")
            
            if result.get("sources"):
                print(f"\nğŸ” ì°¸ê³  ì¶œì²˜:")
                for j, source in enumerate(result["sources"][:3], 1):
                    print(f"  {j}. {source}")
            
            # ì‹¤í–‰ ë¡œê·¸ ìš”ì•½
            quality_assessment = result.get("quality_assessment", {})
            if quality_assessment:
                overall_quality = quality_assessment.get("overall_quality", {})
                print(f"\nğŸ“ˆ í’ˆì§ˆ í‰ê°€:")
                print(f"  ğŸ“„ ë‰´ìŠ¤ í’ˆì§ˆ: {quality_assessment.get('chroma_quality', {}).get('avg_relevance', 0):.3f}")
                print(f"  ğŸ”— ê·¸ë˜í”„ í’ˆì§ˆ: {quality_assessment.get('neo4j_quality', {}).get('result_count', 0)}ê°œ ê´€ê³„")
                print(f"  ğŸ† ì¢…í•© ë“±ê¸‰: {overall_quality.get('quality_tier', 'unknown').upper()}")
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            print(traceback.format_exc())
        
        print("\n" + "=" * 60)
        
        # ë‹¤ìŒ ì§ˆë¬¸ ì „ ì ì‹œ ëŒ€ê¸°
        if i < len(test_questions):
            print("ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™ ì¤‘...")
            time.sleep(1)

if __name__ == "__main__":
    main() 