# langfuse
from langfuse.langchain import CallbackHandler
import os
import argparse
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import json
from collections import defaultdict

from dotenv import load_dotenv
load_dotenv()

# Initialize Langfuse handler only if keys are available
langfuse_handler = None
# if os.getenv('LANGFUSE_PUBLIC_KEY') and os.getenv('LANGFUSE_SECRET_KEY'):
    # langfuse_handler = CallbackHandler()

tri_prompt = """
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ ì „ì²´ë¥¼ ì½ê³ , ì´ ì•ˆì—ì„œ ë°œìƒí•˜ëŠ” í˜„ìƒ, ì›ì¸, ì •ì±…, ê¸°ì—…, ê°œë…, ìƒí’ˆ, ì¸ë¬¼ ë“±ì˜ ì£¼ìš” ê°œì²´ë“¤ ê°„  
ì¸ê³¼ê´€ê³„, ì„¤ëª…, ë¬´ê´€ ê´€ê³„ ë“±ì„ ì¶”ë¡ í•´ì„œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì¤˜.
---
ğŸ¯ ì¶œë ¥ í˜•ì‹ (ì •í™•í•˜ê²Œ ì§€ì¼œì¤˜ì•¼ í•¨):

(ì£¼ì–´:ë…¸ë“œíƒ€ì…)-[ê´€ê³„]->(ëª©ì ì–´:ë…¸ë“œíƒ€ì…)
---
ğŸ“Œ í—ˆìš© ë…¸ë“œ íƒ€ì…:
- Phenomenon: ê´€ì°°ëœ í˜„ìƒ ë˜ëŠ” ê²°ê³¼ (ì˜ˆ: íŒë§¤ ê°ì†Œ, ì£¼ê°€ ìƒìŠ¹)
- Cause: ì›ì¸ì´ ë˜ëŠ” ìš”ì†Œ (ì˜ˆ: ê¸ˆë¦¬ ìƒìŠ¹, ì†Œë¹„ ìœ„ì¶•)
- Concept: ì„¤ëª…ì„ ìœ„í•œ ì¶”ìƒ ê°œë… (ì˜ˆ: êµ¬ë§¤ë ¥, ìˆ˜ìš” ì‹¬ë¦¬)
- Policy: ì •ì±…, ì œë„ (ì˜ˆ: ê¸ˆë¦¬ ì¸ìƒ, ëŒ€ì¶œ ê·œì œ ì™„í™”)
- Product: ìƒí’ˆ, ì„œë¹„ìŠ¤ (ì˜ˆ: í…ŒìŠ¬ë¼ ì°¨ëŸ‰, ì „ê¸°ì°¨)
- Company: ê¸°ì—…, ì¡°ì§ (ì˜ˆ: í…ŒìŠ¬ë¼, ë¡¯ë°)
- Person: ì¸ë¬¼ (ì˜ˆ: ì¼ë¡  ë¨¸ìŠ¤í¬, ì‹ ë™ë¹ˆ)
- Time: íŠ¹ì • ì‹œì  ë˜ëŠ” ì‹œê¸° (ì˜ˆ: 2025ë…„ 7ì›”, ìµœê·¼)
---
ğŸ“Œ í—ˆìš© ê´€ê³„:
- ì›ì¸ì´ë‹¤
- ê²°ê³¼ì´ë‹¤
- ì„¤ëª…í•œë‹¤
- ë¬´ê´€í•˜ë‹¤
- ê´€ë ¨ ìˆë‹¤
- ì†Œì†ì´ë‹¤
- ì˜í–¥ì„ ì¤€ë‹¤
---
âœ‹ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ê·œì¹™:
1. ë‰´ìŠ¤ì— ëª…ì‹œëœ ì •ë³´ì™€, ì¼ë°˜ì  ìƒì‹ì— ê¸°ë°˜í•œ ì¶”ë¡  ê°€ëŠ¥í•œ ê´€ê³„ë§Œ í¬í•¨í•´ì¤˜
2. ê´€ê³„ê°€ ë¶ˆëª…í™•í•˜ê±°ë‚˜ ë¬´ì‘ìœ„ì ì¸ ê²ƒì€ ì œì™¸í•´ì¤˜
3. ë™ì¼ ë¬¸ì„œì—ì„œ ë°œìƒí•œ ë‹¤ì¤‘ ê´€ê³„ëŠ” ëª¨ë‘ í¬í•¨í•´ë„ ë¼ (ì˜ˆ: ì—¬ëŸ¬ ì›ì¸ â†’ í•˜ë‚˜ì˜ ê²°ê³¼)

---

ğŸ“° ë‰´ìŠ¤ ê¸°ì‚¬ ì „ì²´ ë‚´ìš©:

{document_text}
"""

# Global node normalization prompt
global_normalization_prompt = """
ë‹¤ìŒì€ ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì¶”ì¶œëœ ëª¨ë“  ê·¸ë˜í”„ tripletë“¤ì…ë‹ˆë‹¤. 
ì „ì²´ ë°ì´í„°ì…‹ì„ ë¶„ì„í•˜ì—¬ ë™ì¼í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ì§€ë§Œ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë‚˜íƒ€ë‚œ ë…¸ë“œë“¤ì„ ì •ê·œí™”í•´ì£¼ì„¸ìš”.

ì •ê·œí™” ê·œì¹™:
1. ì˜ë¯¸ê°€ ê°™ì€ ë…¸ë“œë“¤ì€ í•˜ë‚˜ì˜ í‘œì¤€ í˜•íƒœë¡œ í†µì¼
2. ê°€ì¥ ê°„ê²°í•˜ê³  ëª…í™•í•œ í‘œí˜„ì„ ì„ íƒ
3. ê¸°ì—…ëª…, ì¸ëª…ì€ ì •í™•í•œ ê³µì‹ ëª…ì¹­ ì‚¬ìš©
4. í˜„ìƒì´ë‚˜ ê°œë…ì€ í•µì‹¬ ì˜ë¯¸ë¥¼ ë‹´ì€ ê°„ê²°í•œ í‘œí˜„ ì‚¬ìš©
5. ë…¸ë“œ íƒ€ì…ì€ ë³€ê²½í•˜ì§€ ì•ŠìŒ
6. ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ ì¼ê´€ì„± ìˆê²Œ ì ìš©

ì „ì²´ tripletë“¤:
{all_triplets}

ì¶œë ¥ í˜•ì‹:
1. ë¨¼ì € ì •ê·œí™” ë§¤í•‘ì„ ì œì‹œí•´ì£¼ì„¸ìš”:
[ì •ê·œí™” ë§¤í•‘]
ì›ë³¸ë…¸ë“œ:íƒ€ì… â†’ ì •ê·œí™”ë…¸ë“œ:íƒ€ì…

2. ê·¸ ë‹¤ìŒ ì •ê·œí™”ëœ ëª¨ë“  tripletë“¤ì„ ì¶œë ¥í•´ì£¼ì„¸ìš”:
[ì •ê·œí™”ëœ Triplets]
(ë…¸ë“œ:íƒ€ì…)-[ê´€ê³„]->(ë…¸ë“œ:íƒ€ì…)
"""

def process_single_article(article, category, article_num, total_articles, chain, langfuse_handler):
    """Process a single article to generate triplets"""
    try:
        print(f"ğŸ”„ Processing article {article_num} in {category}...")
        
        if 'content' not in article:
            print(f"âŒ No content found in article {article_num}")
            return None
            
        document_text = article.get('content', '')
        if not document_text.strip():
            print(f"âŒ Empty content in article {article_num}")
            return None
        
        # Truncate very long articles to speed up processing
        max_chars = 4000  # Increased limit for better quality
        if len(document_text) > max_chars:
            document_text = document_text[:max_chars] + "..."
            print(f"âš ï¸  Article {article_num} truncated to {max_chars} characters")
            
        # Invoke the chain
        config = {}
        if langfuse_handler:
            config["callbacks"] = [langfuse_handler]
        
        result = chain.invoke({"document_text": document_text}, config=config)
        
        # Extract content from result
        if hasattr(result, 'content'):
            triplet_output = result.content
        else:
            triplet_output = str(result)
        
        article_title = article.get('title', 'No title')
        
        # Count triplets generated
        triplet_count = len(re.findall(r'\([^:]+:[^)]+\)-\[[^\]]+\]->\([^:]+:[^)]+\)', triplet_output))
        
        print(f"âœ… Article {article_num}: {article_title[:40]}... ({triplet_count} triplets)")
        
        return {
            'title': article_title,
            'category': category,
            'original': triplet_output,
            'article_id': f"{category}_{article_num}",
            'triplet_count': triplet_count,
            'url': article.get('url', ''),
            'published_date': article.get('published_date', ''),
            'source': article.get('source', '')
        }
        
    except Exception as e:
        print(f"âŒ Error processing article {article_num}: {e}")
        return None

def process_articles_parallel(articles_by_category, chain, langfuse_handler, max_workers=4):
    """Process articles in parallel to speed up triplet generation"""
    all_triplets_data = []
    processed_count = 0
    error_count = 0
    
    # Prepare tasks
    tasks = []
    total_articles = sum(len(articles) for articles in articles_by_category.values())
    
    article_counter = 0
    for category, category_articles in articles_by_category.items():
        for i, article in enumerate(category_articles):
            article_counter += 1
            task = (article, category, article_counter, total_articles, chain, langfuse_handler)
            tasks.append(task)
    
    # Process in parallel
    print(f"ğŸš€ Processing {total_articles} articles with {max_workers} parallel workers...")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_task = {
            executor.submit(process_single_article, *task): task 
            for task in tasks
        }
        
        # Process completed tasks
        for future in as_completed(future_to_task):
            try:
                result = future.result()
                if result:
                    all_triplets_data.append(result)
                    processed_count += 1
                else:
                    error_count += 1
                    
                # Progress indicator
                total_completed = processed_count + error_count
                if total_completed % 3 == 0:
                    print(f"ğŸ“Š Progress: {total_completed}/{total_articles} completed ({(total_completed/total_articles)*100:.1f}%)")
                    
            except Exception as e:
                print(f"âŒ Task failed: {e}")
                error_count += 1
    
    return all_triplets_data, processed_count, error_count

def perform_global_normalization(all_triplets_data, llm):
    """Perform global normalization across all articles"""
    print("\nğŸ”„ Global Node Normalization ì‹œì‘...")
    
    # Combine all triplets
    combined_triplets = ""
    for item in all_triplets_data:
        combined_triplets += item['original'] + "\n"
    
    # Perform global normalization
    global_norm_template = PromptTemplate.from_template(global_normalization_prompt)
    global_norm_chain = global_norm_template | llm
    
    try:
        result = global_norm_chain.invoke({"all_triplets": combined_triplets})
        
        if hasattr(result, 'content'):
            normalized_output = result.content
        else:
            normalized_output = str(result)
        
        return normalized_output
    except Exception as e:
        print(f"Error in global normalization: {e}")
        return combined_triplets

def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='News Triplet Generator')
    parser.add_argument('--limit', '-l', type=int, default=None, 
                       help='Limit number of articles to process (for testing). Use -1 or omit for all articles.')
    parser.add_argument('--categories', '-c', nargs='+', default=None,
                       help='Specific categories to process (e.g., --categories ì •ì¹˜ ê²½ì œ)')
    parser.add_argument('--output', '-o', type=str, default='triplets_output.json',
                       help='Output file name')
    parser.add_argument('--parallel-workers', type=int, default=4,
                       help='Number of parallel workers for article processing (default: 4)')
    parser.add_argument('--skip-normalization', action='store_true',
                       help='Skip global normalization step (faster, but less unified)')
    
    args = parser.parse_args()
    
    # Initialize components
    doc_reasoning_prompt = PromptTemplate.from_template(tri_prompt)
    llm = ChatOpenAI(model="gpt-4.1", temperature=0)
    chain = doc_reasoning_prompt | llm 

    # Read news data
    data_file = '/Users/kai/workspace/rag-tutorial/rag/data/naver_top_news_by_category_20250717_230748.json'
    
    if not os.path.exists(data_file):
        print(f"Data file not found: {data_file}")
        return
    
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"Error reading data file: {e}")
        return

    # Process articles with optional limitations
    if 'articles' not in data:
        print("No articles found in data")
        return
    
    articles = data['articles']
    total_articles = len(articles)
    
    # Filter by categories if specified
    if args.categories:
        filtered_articles = [article for article in articles if article.get('category', 'Unknown') in args.categories]
        print(f"Filtered to {len(filtered_articles)} articles from categories: {args.categories}")
        articles = filtered_articles
        total_articles = len(articles)
    
    # Apply limit if specified
    if args.limit and args.limit > 0:
        articles = articles[:args.limit]
        total_articles = len(articles)
        print(f"ğŸ§ª TEST MODE: Limited to {total_articles} articles")
    elif args.limit == -1:
        print(f"ğŸš€ FULL MODE: Processing all {total_articles} articles")
    else:
        print(f"ğŸš€ FULL MODE: Processing all {total_articles} articles")
    
    print(f"Found {total_articles} articles to process")
    
    # Group articles by category for processing
    articles_by_category = {}
    for article in articles:
        category = article.get('category', 'Unknown')
        if category not in articles_by_category:
            articles_by_category[category] = []
        articles_by_category[category].append(article)
    
    print(f"Categories found: {list(articles_by_category.keys())}")
    print(f"Articles per category: {[(cat, len(arts)) for cat, arts in articles_by_category.items()]}")
    
    processing_mode = "TEST" if args.limit and args.limit > 0 else "FULL"
    print(f"\nğŸ“ Phase 1: {processing_mode} MODE - Triplet ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬, {args.parallel_workers}ê°œ ì›Œì»¤)")
    print("=" * 80)
    
    # Phase 1: Generate triplets from articles using parallel processing
    start_time = time.time()
    all_triplets_data, processed_count, error_count = process_articles_parallel(
        articles_by_category, chain, langfuse_handler, args.parallel_workers
    )
    end_time = time.time()
    
    processing_time = end_time - start_time
    total_triplets = sum([item['triplet_count'] for item in all_triplets_data])
    
    print(f"\nâœ… Phase 1 ì™„ë£Œ ({processing_time:.1f}ì´ˆ):")
    print(f"   - ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ê¸°ì‚¬: {processed_count}/{total_articles}")
    print(f"   - ì˜¤ë¥˜ ë°œìƒ: {error_count}ê°œ")
    print(f"   - ì´ ìƒì„±ëœ triplet: {total_triplets}")
    print(f"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {processing_time/total_articles:.1f}ì´ˆ/ê¸°ì‚¬")
    print(f"   - ì²˜ë¦¬ ì†ë„: {total_articles/processing_time:.1f}ê¸°ì‚¬/ë¶„")
    
    # Save all_triplets_data json list
    
    triplets_output_file = f"triplets_news_data.json"
    with open(triplets_output_file, 'w', encoding='utf-8') as f:
        json.dump(all_triplets_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Triplets data saved to: {triplets_output_file}")
    
    # Phase 2: Global normalization (optional)
    global_normalized_result = None
    if not args.skip_normalization and all_triplets_data:
        print(f"\nğŸ“ Phase 2: ê¸€ë¡œë²Œ ë…¸ë“œ ì •ê·œí™”")
        print("=" * 80)
        
        normalization_start = time.time()
        
        # Count total triplets for optimization decision
        print(f"ğŸ“Š ì´ {total_triplets}ê°œ triplet ì •ê·œí™” ì˜ˆì •")
        
        if total_triplets <= 800:  # Reasonable limit for single LLM call
            print("ğŸ”„ Single global normalization processing...")
            global_normalized_result = perform_global_normalization(all_triplets_data, llm)
        else:
            print(f"âš ï¸  Large dataset ({total_triplets} triplets), using category-based batching...")
            # Category-based processing for large datasets
            category_results = []
            
            categories_triplets = {}
            for item in all_triplets_data:
                cat = item['category']
                if cat not in categories_triplets:
                    categories_triplets[cat] = []
                categories_triplets[cat].append(item)
            
            print(f"ğŸ”„ Processing {len(categories_triplets)} categories separately...")
            
            for category, cat_items in categories_triplets.items():
                cat_triplet_count = sum([item['triplet_count'] for item in cat_items])
                print(f"   Processing {category}: {len(cat_items)} articles, {cat_triplet_count} triplets")
                
                cat_result = perform_global_normalization(cat_items, llm)
                category_results.append(f"=== {category} ===\n{cat_result}")
            
            # Combine category results
            global_normalized_result = "\n\n".join(category_results)
        
        normalization_time = time.time() - normalization_start
        print(f"\nâœ… Phase 2 ì™„ë£Œ ({normalization_time:.1f}ì´ˆ)")
    
    # Save results
    final_results = {
        'processing_summary': {
            'mode': processing_mode,
            'limit_applied': args.limit,
            'categories_filter': args.categories,
            'total_articles_found': len(data['articles']),
            'total_articles_processed': processed_count,
            'total_errors': error_count,
            'success_rate': f"{(processed_count/total_articles)*100:.1f}%",
            'categories': list(set([item['category'] for item in all_triplets_data])),
            'articles_by_category': {cat: len([item for item in all_triplets_data if item['category'] == cat]) 
                                   for cat in set([item['category'] for item in all_triplets_data])},
            'total_triplets_generated': total_triplets,
            'processing_time_seconds': processing_time,
            'average_time_per_article': processing_time/total_articles if total_articles > 0 else 0,
            'articles_per_minute': total_articles/processing_time*60 if processing_time > 0 else 0,
            'normalization_applied': not args.skip_normalization and global_normalized_result is not None,
            'parallel_workers': args.parallel_workers
        },
        'individual_triplets': all_triplets_data,
        'global_normalized_result': global_normalized_result,
        'processing_metadata': {
            'model_used': 'gpt-4.1',
            'max_article_length': 4000,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'command_args': vars(args)
        }
    }
    
    # Save results to file
    output_file = args.output
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ Results saved to {output_file}")
        print(f"   File size: {os.path.getsize(output_file) / 1024 / 1024:.2f} MB")
    except Exception as e:
        print(f"âŒ Error saving results: {e}")
    
    print(f"\nğŸ‰ {processing_mode} ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   ğŸ“Š ì´ {processed_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"   ğŸ“ˆ ì„±ê³µë¥ : {(processed_count/total_articles)*100:.1f}%")
    print(f"   ğŸ”— ì´ ìƒì„±ëœ ê´€ê³„: {total_triplets}ê°œ")
    print(f"   âš¡ ì²˜ë¦¬ ì†ë„: {total_articles/processing_time*60:.1f}ê¸°ì‚¬/ë¶„")

if __name__ == "__main__":
    main() 