# langfuse
from langfuse.langchain import CallbackHandler
import os
import argparse
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import json
from collections import defaultdict

from dotenv import load_dotenv
load_dotenv()

# Initialize Langfuse handler only if keys are available
langfuse_handler = None
# if os.getenv('LANGFUSE_PUBLIC_KEY') and os.getenv('LANGFUSE_SECRET_KEY'):
    # langfuse_handler = CallbackHandler()

tri_prompt = """
Îã§Ïùå Îâ¥Ïä§ Í∏∞ÏÇ¨ Ï†ÑÏ≤¥Î•º ÏùΩÍ≥†, Ïù¥ ÏïàÏóêÏÑú Î∞úÏÉùÌïòÎäî ÌòÑÏÉÅ, ÏõêÏù∏, Ï†ïÏ±Ö, Í∏∞ÏóÖ, Í∞úÎÖê, ÏÉÅÌíà, Ïù∏Î¨º Îì±Ïùò Ï£ºÏöî Í∞úÏ≤¥Îì§ Í∞Ñ  
Ïù∏Í≥ºÍ¥ÄÍ≥Ñ, ÏÑ§Î™Ö, Î¨¥Í¥Ä Í¥ÄÍ≥Ñ Îì±ÏùÑ Ï∂îÎ°†Ìï¥ÏÑú ÏïÑÎûò ÌòïÏãùÏúºÎ°ú Íµ¨Ï°∞ÌôîÌï¥Ï§ò.
---
üéØ Ï∂úÎ†• ÌòïÏãù (Ï†ïÌôïÌïòÍ≤å ÏßÄÏºúÏ§òÏïº Ìï®):

(Ï£ºÏñ¥:ÎÖ∏ÎìúÌÉÄÏûÖ)-[Í¥ÄÍ≥Ñ]->(Î™©Ï†ÅÏñ¥:ÎÖ∏ÎìúÌÉÄÏûÖ)
---
üìå ÌóàÏö© ÎÖ∏Îìú ÌÉÄÏûÖ:
- Phenomenon: Í¥ÄÏ∞∞Îêú ÌòÑÏÉÅ ÎòêÎäî Í≤∞Í≥º (Ïòà: ÌåêÎß§ Í∞êÏÜå, Ï£ºÍ∞Ä ÏÉÅÏäπ)
- Cause: ÏõêÏù∏Ïù¥ ÎêòÎäî ÏöîÏÜå (Ïòà: Í∏àÎ¶¨ ÏÉÅÏäπ, ÏÜåÎπÑ ÏúÑÏ∂ï)
- Concept: ÏÑ§Î™ÖÏùÑ ÏúÑÌïú Ï∂îÏÉÅ Í∞úÎÖê (Ïòà: Íµ¨Îß§Î†•, ÏàòÏöî Ïã¨Î¶¨)
- Policy: Ï†ïÏ±Ö, Ï†úÎèÑ (Ïòà: Í∏àÎ¶¨ Ïù∏ÏÉÅ, ÎåÄÏ∂ú Í∑úÏ†ú ÏôÑÌôî)
- Product: ÏÉÅÌíà, ÏÑúÎπÑÏä§ (Ïòà: ÌÖåÏä¨Îùº Ï∞®Îüâ, Ï†ÑÍ∏∞Ï∞®)
- Company: Í∏∞ÏóÖ, Ï°∞ÏßÅ (Ïòà: ÌÖåÏä¨Îùº, Î°ØÎç∞)
- Person: Ïù∏Î¨º (Ïòà: ÏùºÎ°† Î®∏Ïä§ÌÅ¨, Ïã†ÎèôÎπà)
- Time: ÌäπÏ†ï ÏãúÏ†ê ÎòêÎäî ÏãúÍ∏∞ (Ïòà: 2025ÎÖÑ 7Ïõî, ÏµúÍ∑º)
---
üìå ÌóàÏö© Í¥ÄÍ≥Ñ:
- ÏõêÏù∏Ïù¥Îã§
- Í≤∞Í≥ºÏù¥Îã§
- ÏÑ§Î™ÖÌïúÎã§
- Î¨¥Í¥ÄÌïòÎã§
- Í¥ÄÎ†® ÏûàÎã§
- ÏÜåÏÜçÏù¥Îã§
- ÏòÅÌñ•ÏùÑ Ï§ÄÎã§
---
‚úã Î∞òÎìúÏãú ÏßÄÏºúÏïº Ìï† Í∑úÏπô:
1. Îâ¥Ïä§Ïóê Î™ÖÏãúÎêú Ï†ïÎ≥¥ÏôÄ, ÏùºÎ∞òÏ†Å ÏÉÅÏãùÏóê Í∏∞Î∞òÌïú Ï∂îÎ°† Í∞ÄÎä•Ìïú Í¥ÄÍ≥ÑÎßå Ìè¨Ìï®Ìï¥Ï§ò
2. Í¥ÄÍ≥ÑÍ∞Ä Î∂àÎ™ÖÌôïÌïòÍ±∞ÎÇò Î¨¥ÏûëÏúÑÏ†ÅÏù∏ Í≤ÉÏùÄ Ï†úÏô∏Ìï¥Ï§ò
3. ÎèôÏùº Î¨∏ÏÑúÏóêÏÑú Î∞úÏÉùÌïú Îã§Ï§ë Í¥ÄÍ≥ÑÎäî Î™®Îëê Ìè¨Ìï®Ìï¥ÎèÑ Îèº (Ïòà: Ïó¨Îü¨ ÏõêÏù∏ ‚Üí ÌïòÎÇòÏùò Í≤∞Í≥º)

---

üì∞ Îâ¥Ïä§ Í∏∞ÏÇ¨ Ï†ÑÏ≤¥ ÎÇ¥Ïö©:

{document_text}
"""

# Global node normalization prompt
global_normalization_prompt = """
Îã§ÏùåÏùÄ Ïó¨Îü¨ Îâ¥Ïä§ Í∏∞ÏÇ¨ÏóêÏÑú Ï∂îÏ∂úÎêú Î™®Îì† Í∑∏ÎûòÌîÑ tripletÎì§ÏûÖÎãàÎã§. 
Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÎèôÏùºÌïú ÏùòÎØ∏Î•º Í∞ÄÏßÄÏßÄÎßå Îã§Î•∏ ÌëúÌòÑÏúºÎ°ú ÎÇòÌÉÄÎÇú ÎÖ∏ÎìúÎì§ÏùÑ Ï†ïÍ∑úÌôîÌï¥Ï£ºÏÑ∏Ïöî.

Ï†ïÍ∑úÌôî Í∑úÏπô:
1. ÏùòÎØ∏Í∞Ä Í∞ôÏùÄ ÎÖ∏ÎìúÎì§ÏùÄ ÌïòÎÇòÏùò ÌëúÏ§Ä ÌòïÌÉúÎ°ú ÌÜµÏùº
2. Í∞ÄÏû• Í∞ÑÍ≤∞ÌïòÍ≥† Î™ÖÌôïÌïú ÌëúÌòÑÏùÑ ÏÑ†ÌÉù
3. Í∏∞ÏóÖÎ™Ö, Ïù∏Î™ÖÏùÄ Ï†ïÌôïÌïú Í≥µÏãù Î™ÖÏπ≠ ÏÇ¨Ïö©
4. ÌòÑÏÉÅÏù¥ÎÇò Í∞úÎÖêÏùÄ ÌïµÏã¨ ÏùòÎØ∏Î•º Îã¥ÏùÄ Í∞ÑÍ≤∞Ìïú ÌëúÌòÑ ÏÇ¨Ïö©
5. ÎÖ∏Îìú ÌÉÄÏûÖÏùÄ Î≥ÄÍ≤ΩÌïòÏßÄ ÏïäÏùå
6. Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏóêÏÑú ÏùºÍ¥ÄÏÑ± ÏûàÍ≤å Ï†ÅÏö©

Ï†ÑÏ≤¥ tripletÎì§:
{all_triplets}

Ï∂úÎ†• ÌòïÏãù:
1. Î®ºÏ†Ä Ï†ïÍ∑úÌôî Îß§ÌïëÏùÑ Ï†úÏãúÌï¥Ï£ºÏÑ∏Ïöî:
[Ï†ïÍ∑úÌôî Îß§Ìïë]
ÏõêÎ≥∏ÎÖ∏Îìú:ÌÉÄÏûÖ ‚Üí Ï†ïÍ∑úÌôîÎÖ∏Îìú:ÌÉÄÏûÖ

2. Í∑∏ Îã§Ïùå Ï†ïÍ∑úÌôîÎêú Î™®Îì† tripletÎì§ÏùÑ Ï∂úÎ†•Ìï¥Ï£ºÏÑ∏Ïöî:
[Ï†ïÍ∑úÌôîÎêú Triplets]
(ÎÖ∏Îìú:ÌÉÄÏûÖ)-[Í¥ÄÍ≥Ñ]->(ÎÖ∏Îìú:ÌÉÄÏûÖ)
"""

def process_single_article(article, category, article_num, total_articles, chain, langfuse_handler):
    """Process a single article to generate triplets"""
    try:
        print(f"üîÑ Processing article {article_num} in {category}...")
        
        if 'content' not in article:
            print(f"‚ùå No content found in article {article_num}")
            return None
            
        document_text = article.get('content', '')
        if not document_text.strip():
            print(f"‚ùå Empty content in article {article_num}")
            return None
        
        # Truncate very long articles to speed up processing
        max_chars = 4000  # Increased limit for better quality
        if len(document_text) > max_chars:
            document_text = document_text[:max_chars] + "..."
            print(f"‚ö†Ô∏è  Article {article_num} truncated to {max_chars} characters")
            
        # Invoke the chain
        config = {}
        if langfuse_handler:
            config["callbacks"] = [langfuse_handler]
        
        result = chain.invoke({"document_text": document_text}, config=config)
        
        # Extract content from result
        if hasattr(result, 'content'):
            triplet_output = result.content
        else:
            triplet_output = str(result)
        
        article_title = article.get('title', 'No title')
        
        # Count triplets generated
        triplet_count = len(re.findall(r'\([^:]+:[^)]+\)-\[[^\]]+\]->\([^:]+:[^)]+\)', triplet_output))
        
        print(f"‚úÖ Article {article_num}: {article_title[:40]}... ({triplet_count} triplets)")
        
        return {
            'title': article_title,
            'category': category,
            'original': triplet_output,
            'article_id': f"{category}_{article_num}",
            'triplet_count': triplet_count,
            'url': article.get('url', ''),
            'published_date': article.get('published_date', ''),
            'source': article.get('source', '')
        }
        
    except Exception as e:
        print(f"‚ùå Error processing article {article_num}: {e}")
        return None

def process_articles_parallel(articles_by_category, chain, langfuse_handler, max_workers=4):
    """Process articles in parallel to speed up triplet generation"""
    all_triplets_data = []
    processed_count = 0
    error_count = 0
    
    # Prepare tasks
    tasks = []
    total_articles = sum(len(articles) for articles in articles_by_category.values())
    
    article_counter = 0
    for category, category_articles in articles_by_category.items():
        for i, article in enumerate(category_articles):
            article_counter += 1
            task = (article, category, article_counter, total_articles, chain, langfuse_handler)
            tasks.append(task)
    
    # Process in parallel
    print(f"üöÄ Processing {total_articles} articles with {max_workers} parallel workers...")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_task = {
            executor.submit(process_single_article, *task): task 
            for task in tasks
        }
        
        # Process completed tasks
        for future in as_completed(future_to_task):
            try:
                result = future.result()
                if result:
                    all_triplets_data.append(result)
                    processed_count += 1
                else:
                    error_count += 1
                    
                # Progress indicator
                total_completed = processed_count + error_count
                if total_completed % 3 == 0:
                    print(f"üìä Progress: {total_completed}/{total_articles} completed ({(total_completed/total_articles)*100:.1f}%)")
                    
            except Exception as e:
                print(f"‚ùå Task failed: {e}")
                error_count += 1
    
    return all_triplets_data, processed_count, error_count

def perform_global_normalization(all_triplets_data, llm):
    """Perform global normalization across all articles"""
    print("\nüîÑ Global Node Normalization ÏãúÏûë...")
    
    # Combine all triplets
    combined_triplets = ""
    for item in all_triplets_data:
        combined_triplets += item['original'] + "\n"
    
    # Perform global normalization
    global_norm_template = PromptTemplate.from_template(global_normalization_prompt)
    global_norm_chain = global_norm_template | llm
    
    try:
        result = global_norm_chain.invoke({"all_triplets": combined_triplets})
        
        if hasattr(result, 'content'):
            normalized_output = result.content
        else:
            normalized_output = str(result)
        
        return normalized_output
    except Exception as e:
        print(f"Error in global normalization: {e}")
        return combined_triplets

def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='News Triplet Generator')
    parser.add_argument('--limit', '-l', type=int, default=None, 
                       help='Limit number of articles to process (for testing). Use -1 or omit for all articles.')
    parser.add_argument('--categories', '-c', nargs='+', default=None,
                       help='Specific categories to process (e.g., --categories Ï†ïÏπò Í≤ΩÏ†ú)')
    parser.add_argument('--output', '-o', type=str, default='triplets_output.json',
                       help='Output file name')
    parser.add_argument('--parallel-workers', type=int, default=4,
                       help='Number of parallel workers for article processing (default: 4)')
    parser.add_argument('--skip-normalization', action='store_true',
                       help='Skip global normalization step (faster, but less unified)')
    
    args = parser.parse_args()
    
    # Initialize components
    doc_reasoning_prompt = PromptTemplate.from_template(tri_prompt)
    llm = ChatOpenAI(model="gpt-4.1", temperature=0)
    chain = doc_reasoning_prompt | llm 

    # Read news data
    data_file = '/Users/kai/workspace/rag-tutorial/rag/data/naver_top_news_by_category_20250717_230748.json'
    
    if not os.path.exists(data_file):
        print(f"Data file not found: {data_file}")
        return
    
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"Error reading data file: {e}")
        return

    # Process articles with optional limitations
    if 'articles' not in data:
        print("No articles found in data")
        return
    
    articles = data['articles']
    total_articles = len(articles)
    
    # Filter by categories if specified
    if args.categories:
        filtered_articles = [article for article in articles if article.get('category', 'Unknown') in args.categories]
        print(f"Filtered to {len(filtered_articles)} articles from categories: {args.categories}")
        articles = filtered_articles
        total_articles = len(articles)
    
    # Apply limit if specified
    if args.limit and args.limit > 0:
        articles = articles[:args.limit]
        total_articles = len(articles)
        print(f"üß™ TEST MODE: Limited to {total_articles} articles")
    elif args.limit == -1:
        print(f"üöÄ FULL MODE: Processing all {total_articles} articles")
    else:
        print(f"üöÄ FULL MODE: Processing all {total_articles} articles")
    
    print(f"Found {total_articles} articles to process")
    
    # Group articles by category for processing
    articles_by_category = {}
    for article in articles:
        category = article.get('category', 'Unknown')
        if category not in articles_by_category:
            articles_by_category[category] = []
        articles_by_category[category].append(article)
    
    print(f"Categories found: {list(articles_by_category.keys())}")
    print(f"Articles per category: {[(cat, len(arts)) for cat, arts in articles_by_category.items()]}")
    
    processing_mode = "TEST" if args.limit and args.limit > 0 else "FULL"
    print(f"\nüìù Phase 1: {processing_mode} MODE - Triplet ÏÉùÏÑ± (Î≥ëÎ†¨ Ï≤òÎ¶¨, {args.parallel_workers}Í∞ú ÏõåÏª§)")
    print("=" * 80)
    
    # Phase 1: Generate triplets from articles using parallel processing
    start_time = time.time()
    all_triplets_data, processed_count, error_count = process_articles_parallel(
        articles_by_category, chain, langfuse_handler, args.parallel_workers
    )
    end_time = time.time()
    
    processing_time = end_time - start_time
    total_triplets = sum([item['triplet_count'] for item in all_triplets_data])
    
    print(f"\n‚úÖ Phase 1 ÏôÑÎ£å ({processing_time:.1f}Ï¥à):")
    print(f"   - ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨Îêú Í∏∞ÏÇ¨: {processed_count}/{total_articles}")
    print(f"   - Ïò§Î•ò Î∞úÏÉù: {error_count}Í∞ú")
    print(f"   - Ï¥ù ÏÉùÏÑ±Îêú triplet: {total_triplets}")
    print(f"   - ÌèâÍ∑† Ï≤òÎ¶¨ ÏãúÍ∞Ñ: {processing_time/total_articles:.1f}Ï¥à/Í∏∞ÏÇ¨")
    print(f"   - Ï≤òÎ¶¨ ÏÜçÎèÑ: {total_articles/processing_time:.1f}Í∏∞ÏÇ¨/Î∂Ñ")
    
    # Save all_triplets_data json list
    
    triplets_output_file = f"triplets_news_data.json"
    with open(triplets_output_file, 'w', encoding='utf-8') as f:
        json.dump(all_triplets_data, f, ensure_ascii=False, indent=2)
    print(f"üíæ Triplets data saved to: {triplets_output_file}")
    
    # Phase 2: Global normalization (optional)
    global_normalized_result = None
    if not args.skip_normalization and all_triplets_data:
        print(f"\nüìù Phase 2: Í∏ÄÎ°úÎ≤å ÎÖ∏Îìú Ï†ïÍ∑úÌôî")
        print("=" * 80)
        
        normalization_start = time.time()
        
        # Count total triplets for optimization decision
        print(f"üìä Ï¥ù {total_triplets}Í∞ú triplet Ï†ïÍ∑úÌôî ÏòàÏ†ï")
        
        if total_triplets <= 800:  # Reasonable limit for single LLM call
            print("üîÑ Single global normalization processing...")
            global_normalized_result = perform_global_normalization(all_triplets_data, llm)
        else:
            print(f"‚ö†Ô∏è  Large dataset ({total_triplets} triplets), using category-based batching...")
            # Category-based processing for large datasets
            category_results = []
            
            categories_triplets = {}
            for item in all_triplets_data:
                cat = item['category']
                if cat not in categories_triplets:
                    categories_triplets[cat] = []
                categories_triplets[cat].append(item)
            
            print(f"üîÑ Processing {len(categories_triplets)} categories separately...")
            
            for category, cat_items in categories_triplets.items():
                cat_triplet_count = sum([item['triplet_count'] for item in cat_items])
                print(f"   Processing {category}: {len(cat_items)} articles, {cat_triplet_count} triplets")
                
                cat_result = perform_global_normalization(cat_items, llm)
                category_results.append(f"=== {category} ===\n{cat_result}")
            
            # Combine category results
            global_normalized_result = "\n\n".join(category_results)
        
        normalization_time = time.time() - normalization_start
        print(f"\n‚úÖ Phase 2 ÏôÑÎ£å ({normalization_time:.1f}Ï¥à)")
    
    # Save results
    final_results = {
        'processing_summary': {
            'mode': processing_mode,
            'limit_applied': args.limit,
            'categories_filter': args.categories,
            'total_articles_found': len(data['articles']),
            'total_articles_processed': processed_count,
            'total_errors': error_count,
            'success_rate': f"{(processed_count/total_articles)*100:.1f}%",
            'categories': list(set([item['category'] for item in all_triplets_data])),
            'articles_by_category': {cat: len([item for item in all_triplets_data if item['category'] == cat]) 
                                   for cat in set([item['category'] for item in all_triplets_data])},
            'total_triplets_generated': total_triplets,
            'processing_time_seconds': processing_time,
            'average_time_per_article': processing_time/total_articles if total_articles > 0 else 0,
            'articles_per_minute': total_articles/processing_time*60 if processing_time > 0 else 0,
            'normalization_applied': not args.skip_normalization and global_normalized_result is not None,
            'parallel_workers': args.parallel_workers
        },
        'individual_triplets': all_triplets_data,
        'global_normalized_result': global_normalized_result,
        'processing_metadata': {
            'model_used': 'gpt-4.1',
            'max_article_length': 4000,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'command_args': vars(args)
        }
    }
    
    # Save results to file
    output_file = args.output
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        print(f"\nüíæ Results saved to {output_file}")
        print(f"   File size: {os.path.getsize(output_file) / 1024 / 1024:.2f} MB")
    except Exception as e:
        print(f"‚ùå Error saving results: {e}")
    
    print(f"\nüéâ {processing_mode} Ï≤òÎ¶¨ ÏôÑÎ£å!")
    print(f"   üìä Ï¥ù {processed_count}Í∞ú Í∏∞ÏÇ¨ Ï≤òÎ¶¨ ÏôÑÎ£å")
    print(f"   üìà ÏÑ±Í≥µÎ•†: {(processed_count/total_articles)*100:.1f}%")
    print(f"   üîó Ï¥ù ÏÉùÏÑ±Îêú Í¥ÄÍ≥Ñ: {total_triplets}Í∞ú")
    print(f"   ‚ö° Ï≤òÎ¶¨ ÏÜçÎèÑ: {total_articles/processing_time*60:.1f}Í∏∞ÏÇ¨/Î∂Ñ")

if __name__ == "__main__":
    main() 