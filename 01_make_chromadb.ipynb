{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204eedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 네이버 뉴스 데이터를 Chroma DB에 저장하는 파이프라인\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 환경 변수 설정 (OpenAI API 키가 필요합니다)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"  # 실제 API 키로 변경해주세요\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# Langfuse 콜백 핸들러 생성\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "\n",
    "print(\"라이브러리 로드 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 파일 읽기\n",
    "import json\n",
    "\n",
    "with open('data/naver_news.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "news_urls = [article['url'] for article in data['articles']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9adc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 네이버 뉴스 데이터 로드 및 전처리 함수\n",
    "def load_and_extract_naver_news(json_file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    네이버 뉴스 JSON 파일에서 필요한 필드만 추출\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: JSON 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        추출된 뉴스 데이터 리스트\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    extracted_articles = []\n",
    "    \n",
    "    for article in data.get('articles', []):\n",
    "        # 필요한 필드만 추출\n",
    "        extracted_article = {\n",
    "            'title': article.get('title', ''),\n",
    "            'content': article.get('content', ''),\n",
    "            'url': article.get('url', ''),\n",
    "            'published_date': article.get('published_date', ''),\n",
    "            'author': article.get('author', ''),\n",
    "            'source': article.get('source', ''),\n",
    "            'category': article.get('category', ''),\n",
    "            'crawled_at': article.get('crawled_at', ''),\n",
    "        }\n",
    "        \n",
    "        # 빈 컨텐츠는 제외\n",
    "        if extracted_article['title'] and extracted_article['content']:\n",
    "            extracted_articles.append(extracted_article)\n",
    "    \n",
    "    print(f\"총 {len(extracted_articles)}개의 뉴스 기사를 추출했습니다.\")\n",
    "    return extracted_articles\n",
    "\n",
    "# 데이터 로드 테스트\n",
    "json_file_path = \"../data/naver_top_news_by_category_20250717_230748.json\"\n",
    "extracted_news = load_and_extract_naver_news(json_file_path)\n",
    "\n",
    "# 첫 번째 기사 확인\n",
    "if extracted_news:\n",
    "    first_article = extracted_news[0]\n",
    "    print(\"\\n첫 번째 기사 예시:\")\n",
    "    print(f\"제목: {first_article['title'][:100]}...\")\n",
    "    print(f\"내용: {first_article['content'][:200]}...\")\n",
    "    print(f\"URL: {first_article['url']}\")\n",
    "    print(f\"발행일: {first_article['published_date']}\") \n",
    "else:\n",
    "    print(\"추출된 기사가 없습니다.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1643930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents_from_news(extracted_news: List[Dict[str, Any]]) -> List[Document]:\n",
    "    documents = []\n",
    "    \n",
    "    for i, article in enumerate(extracted_news):\n",
    "        # 뉴스 본문 문단 단위로 분할\n",
    "        content = article['content']\n",
    "        title = article['title']\n",
    "        full_text = f\"{title}\\n\\n{content}\"\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            encoding_name=\"cl100k_base\",\n",
    "            separators=['\\n\\n', '\\n', r'(?<=[.!?])\\s+'],\n",
    "            chunk_size=300,\n",
    "            chunk_overlap=50,\n",
    "            is_separator_regex=True,\n",
    "            keep_separator=True,\n",
    "        )\n",
    "        \n",
    "        chunks = text_splitter.split_text(full_text)\n",
    "        \n",
    "        for j, chunk in enumerate(chunks):\n",
    "            if chunk.strip():\n",
    "                metadata = {\n",
    "                    'category': article['category'],\n",
    "                    'author': article['author'],\n",
    "                    'source': article['source'],\n",
    "                    'title': title,\n",
    "                    'url': article['url'],\n",
    "                    'published_date': article['published_date'],\n",
    "                    'article_id': i,\n",
    "                    'chunk_id': f\"{i}-{j}\",  # 고유 chunk ID\n",
    "                }\n",
    "                \n",
    "                documents.append(Document(\n",
    "                    page_content=chunk.strip(),\n",
    "                    metadata=metadata\n",
    "                ))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "# 문서 생성 테스트\n",
    "documents = create_documents_from_news(extracted_news)\n",
    "print(f\"\\n총 {len(documents)}개의 문서 청크가 생성되었습니다.\")\n",
    "\n",
    "if documents:\n",
    "    print(f\"\\n첫 번째 문서 청크 예시:\")\n",
    "    print(f\"내용: {documents[0].page_content[:200]}...\")\n",
    "    print(f\"메타데이터: {documents[0].metadata}\")\n",
    "    print(f\"메타데이터: {documents[1].metadata}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e68dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Chroma DB에 저장\n",
    "def save_to_chroma_db(documents: List[Document], collection_name: str = \"naver_news\"):\n",
    "    \"\"\"\n",
    "    문서들을 Chroma DB에 저장\n",
    "    \n",
    "    Args:\n",
    "        documents: 저장할 Document 객체 리스트\n",
    "        collection_name: 컬렉션 이름\n",
    "    \"\"\"\n",
    "    # OpenAI Embeddings 초기화 (large 모델 사용)\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        # OpenAI API 키는 환경 변수에서 자동으로 읽어옵니다\n",
    "    )\n",
    "    \n",
    "    # Chroma DB 저장 경로\n",
    "    persist_directory = \"../chroma_db_news_3\"\n",
    "    \n",
    "    # Chroma 벡터 스토어 생성\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ {len(documents)}개의 문서가 Chroma DB에 저장되었습니다.\")\n",
    "    print(f\"📁 저장 경로: {persist_directory}\")\n",
    "    print(f\"📋 컬렉션 이름: {collection_name}\")\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# 주의: 실제 실행하려면 OpenAI API 키가 필요합니다\n",
    "vectorstore = save_to_chroma_db(documents)\n",
    "print(\"Chroma DB 저장 함수가 준비되었습니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code 작성\n",
    "# langchain langfuse 사용\n",
    "# 벡터 저장소 로드 \n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"naver_news\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"chroma_db_news_3\",\n",
    ")\n",
    "# 저장된 문서 수 확인\n",
    "print(\"Chroma DB에 저장된 문서 수:\", chroma_db._collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7868944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색기 지정하여 테스트 \n",
    "chroma_k_retriever = chroma_db.as_retriever(\n",
    "    search_kwargs={\"k\": 5},\n",
    ")\n",
    "\n",
    "query = \"금리 인하\"\n",
    "retrieved_docs = chroma_k_retriever.invoke(query)\n",
    "print(retrieved_docs)\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)\n",
    "    import pprint\n",
    "    pprint.pprint(doc)\n",
    "    print(\"=\"*200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
