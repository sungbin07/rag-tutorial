{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204eedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ Chroma DBì— ì €ìž¥í•˜ëŠ” íŒŒì´í”„ë¼ì¸\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"  # ì‹¤ì œ API í‚¤ë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# Langfuse ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json íŒŒì¼ ì½ê¸°\n",
    "import json\n",
    "\n",
    "with open('data/naver_news.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "news_urls = [article['url'] for article in data['articles']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9adc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def load_and_extract_naver_news(json_file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    ë„¤ì´ë²„ ë‰´ìŠ¤ JSON íŒŒì¼ì—ì„œ í•„ìš”í•œ í•„ë“œë§Œ ì¶”ì¶œ\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: JSON íŒŒì¼ ê²½ë¡œ\n",
    "        \n",
    "    Returns:\n",
    "        ì¶”ì¶œëœ ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    extracted_articles = []\n",
    "    \n",
    "    for article in data.get('articles', []):\n",
    "        # í•„ìš”í•œ í•„ë“œë§Œ ì¶”ì¶œ\n",
    "        extracted_article = {\n",
    "            'title': article.get('title', ''),\n",
    "            'content': article.get('content', ''),\n",
    "            'url': article.get('url', ''),\n",
    "            'published_date': article.get('published_date', ''),\n",
    "            'author': article.get('author', ''),\n",
    "            'source': article.get('source', ''),\n",
    "            'category': article.get('category', ''),\n",
    "            'crawled_at': article.get('crawled_at', ''),\n",
    "        }\n",
    "        \n",
    "        # ë¹ˆ ì»¨í…ì¸ ëŠ” ì œì™¸\n",
    "        if extracted_article['title'] and extracted_article['content']:\n",
    "            extracted_articles.append(extracted_article)\n",
    "    \n",
    "    print(f\"ì´ {len(extracted_articles)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    return extracted_articles\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸\n",
    "json_file_path = \"../data/naver_top_news_by_category_20250717_230748.json\"\n",
    "extracted_news = load_and_extract_naver_news(json_file_path)\n",
    "\n",
    "# ì²« ë²ˆì§¸ ê¸°ì‚¬ í™•ì¸\n",
    "if extracted_news:\n",
    "    first_article = extracted_news[0]\n",
    "    print(\"\\nì²« ë²ˆì§¸ ê¸°ì‚¬ ì˜ˆì‹œ:\")\n",
    "    print(f\"ì œëª©: {first_article['title'][:100]}...\")\n",
    "    print(f\"ë‚´ìš©: {first_article['content'][:200]}...\")\n",
    "    print(f\"URL: {first_article['url']}\")\n",
    "    print(f\"ë°œí–‰ì¼: {first_article['published_date']}\") \n",
    "else:\n",
    "    print(\"ì¶”ì¶œëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1643930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents_from_news(extracted_news: List[Dict[str, Any]]) -> List[Document]:\n",
    "    documents = []\n",
    "    \n",
    "    for i, article in enumerate(extracted_news):\n",
    "        # ë‰´ìŠ¤ ë³¸ë¬¸ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "        content = article['content']\n",
    "        title = article['title']\n",
    "        full_text = f\"{title}\\n\\n{content}\"\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            encoding_name=\"cl100k_base\",\n",
    "            separators=['\\n\\n', '\\n', r'(?<=[.!?])\\s+'],\n",
    "            chunk_size=300,\n",
    "            chunk_overlap=50,\n",
    "            is_separator_regex=True,\n",
    "            keep_separator=True,\n",
    "        )\n",
    "        \n",
    "        chunks = text_splitter.split_text(full_text)\n",
    "        \n",
    "        for j, chunk in enumerate(chunks):\n",
    "            if chunk.strip():\n",
    "                metadata = {\n",
    "                    'category': article['category'],\n",
    "                    'author': article['author'],\n",
    "                    'source': article['source'],\n",
    "                    'title': title,\n",
    "                    'url': article['url'],\n",
    "                    'published_date': article['published_date'],\n",
    "                    'article_id': i,\n",
    "                    'chunk_id': f\"{i}-{j}\",  # ê³ ìœ  chunk ID\n",
    "                }\n",
    "                \n",
    "                documents.append(Document(\n",
    "                    page_content=chunk.strip(),\n",
    "                    metadata=metadata\n",
    "                ))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "documents = create_documents_from_news(extracted_news)\n",
    "print(f\"\\nì´ {len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if documents:\n",
    "    print(f\"\\nì²« ë²ˆì§¸ ë¬¸ì„œ ì²­í¬ ì˜ˆì‹œ:\")\n",
    "    print(f\"ë‚´ìš©: {documents[0].page_content[:200]}...\")\n",
    "    print(f\"ë©”íƒ€ë°ì´í„°: {documents[0].metadata}\")\n",
    "    print(f\"ë©”íƒ€ë°ì´í„°: {documents[1].metadata}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e68dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Chroma DBì— ì €ìž¥\n",
    "def save_to_chroma_db(documents: List[Document], collection_name: str = \"naver_news\"):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œë“¤ì„ Chroma DBì— ì €ìž¥\n",
    "    \n",
    "    Args:\n",
    "        documents: ì €ìž¥í•  Document ê°ì²´ ë¦¬ìŠ¤íŠ¸\n",
    "        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„\n",
    "    \"\"\"\n",
    "    # OpenAI Embeddings ì´ˆê¸°í™” (large ëª¨ë¸ ì‚¬ìš©)\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        # OpenAI API í‚¤ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ ìžë™ìœ¼ë¡œ ì½ì–´ì˜µë‹ˆë‹¤\n",
    "    )\n",
    "    \n",
    "    # Chroma DB ì €ìž¥ ê²½ë¡œ\n",
    "    persist_directory = \"../chroma_db_news_3\"\n",
    "    \n",
    "    # Chroma ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… {len(documents)}ê°œì˜ ë¬¸ì„œê°€ Chroma DBì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ðŸ“ ì €ìž¥ ê²½ë¡œ: {persist_directory}\")\n",
    "    print(f\"ðŸ“‹ ì»¬ë ‰ì…˜ ì´ë¦„: {collection_name}\")\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# ì£¼ì˜: ì‹¤ì œ ì‹¤í–‰í•˜ë ¤ë©´ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤\n",
    "vectorstore = save_to_chroma_db(documents)\n",
    "print(\"Chroma DB ì €ìž¥ í•¨ìˆ˜ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code ìž‘ì„±\n",
    "# langchain langfuse ì‚¬ìš©\n",
    "# ë²¡í„° ì €ìž¥ì†Œ ë¡œë“œ \n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"naver_news\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"chroma_db_news_3\",\n",
    ")\n",
    "# ì €ìž¥ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸\n",
    "print(\"Chroma DBì— ì €ìž¥ëœ ë¬¸ì„œ ìˆ˜:\", chroma_db._collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7868944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ìƒ‰ê¸° ì§€ì •í•˜ì—¬ í…ŒìŠ¤íŠ¸ \n",
    "chroma_k_retriever = chroma_db.as_retriever(\n",
    "    search_kwargs={\"k\": 5},\n",
    ")\n",
    "\n",
    "query = \"ê¸ˆë¦¬ ì¸í•˜\"\n",
    "retrieved_docs = chroma_k_retriever.invoke(query)\n",
    "print(retrieved_docs)\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)\n",
    "    import pprint\n",
    "    pprint.pprint(doc)\n",
    "    print(\"=\"*200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
