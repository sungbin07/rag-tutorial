#!/usr/bin/env python3
"""
Tool-based Hybrid RAG Agent: ChromaDB + Neo4j + LangGraph with Tools
LLMì´ ë™ì ìœ¼ë¡œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ê²€ìƒ‰í•˜ê³  ì¶”ë¡ í•˜ëŠ” Agent ì‹œìŠ¤í…œ
"""

from typing import TypedDict, List, Dict, Optional, Annotated
import json
import time
from datetime import datetime
import os
import asyncio

# LangChain imports
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.tools import tool
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate

# LangGraph imports  
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph.message import add_messages

# Database imports
import chromadb
from neo4j import GraphDatabase

# Environment and logging
from dotenv import load_dotenv
from langfuse.langchain import CallbackHandler

load_dotenv()

# Langfuse setup
langfuse_handler = CallbackHandler() if os.getenv('LANGFUSE_PUBLIC_KEY') else None

# State definition with message passing
class ToolAgentState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    question: str
    search_results: Dict
    final_answer: str
    confidence_score: float
    sources: List[str]
    execution_log: List[Dict]

# Tool functions
@tool
def analyze_question_tool(question: str) -> str:
    """
    ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ì „ëµê³¼ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        
    Returns:
        JSON í˜•ì‹ì˜ ë¶„ì„ ê²°ê³¼ (ì§ˆë¬¸ ìœ í˜•, í‚¤ì›Œë“œ, ê²€ìƒ‰ ì „ëµ ë“±)
    """
    try:
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        
        analysis_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”:

ì§ˆë¬¸: {question}

ë¶„ì„ í•­ëª©:
1. ì§ˆë¬¸ ìœ í˜• (ì‚¬ì‹¤ í™•ì¸, ì¸ê³¼ê´€ê³„, í˜„í™© ë¶„ì„, ì˜ˆì¸¡ ë“±)
2. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (3-5ê°œ)
3. ì‹œê°„ì  ë²”ìœ„ (ìµœì‹ , íŠ¹ì • ê¸°ê°„, ì¼ë°˜ì )
4. ê²€ìƒ‰ ìš°ì„ ìˆœìœ„ (ë‰´ìŠ¤ ë‚´ìš© vs ê´€ê³„ ê·¸ë˜í”„)
5. ì˜ˆìƒ ë‹µë³€ ë³µì¡ë„ (ë‹¨ìˆœ/ë³µí•©)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "question_type": "ì§ˆë¬¸ ìœ í˜•",
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
    "time_scope": "ì‹œê°„ì  ë²”ìœ„",
    "search_priority": "chroma_first|neo4j_first|parallel",
    "complexity": "simple|complex",
    "reasoning": "ì „ëµ ì„ íƒ ì´ìœ "
}}
"""
        
        config = {"callbacks": [langfuse_handler]} if langfuse_handler else {}
        response = llm.invoke(analysis_prompt, config=config)
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            strategy = json.loads(response.content)
        except:
            # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì „ëµ
            strategy = {
                "question_type": "ì¼ë°˜ ì§ˆë¬¸",
                "keywords": question.split()[:3],
                "time_scope": "ìµœì‹ ",
                "search_priority": "parallel",
                "complexity": "complex",
                "reasoning": "ìë™ ë¶„ì„ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ì „ëµ ì ìš©"
            }
        
        print(f"ğŸ¯ ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ: {strategy['question_type']}")
        print(f"ğŸ”‘ í‚¤ì›Œë“œ: {', '.join(strategy['keywords'])}")
        print(f"ğŸ¯ ê²€ìƒ‰ ì „ëµ: {strategy['search_priority']}")
        
        return json.dumps(strategy, ensure_ascii=False)
        
    except Exception as e:
        error_result = {
            "question_type": "ì˜¤ë¥˜",
            "keywords": question.split()[:3],
            "search_priority": "parallel",
            "complexity": "complex",
            "reasoning": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        }
        return json.dumps(error_result, ensure_ascii=False)

@tool
def search_chroma_news_tool(query: str, keywords: Optional[str] = None) -> str:
    """
    ChromaDBì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰í•  ì§ˆë¬¸ì´ë‚˜ ì¿¼ë¦¬
        keywords: ì¶”ê°€ í‚¤ì›Œë“œ (JSON ë¬¸ìì—´ í˜•íƒœ)
        
    Returns:
        JSON í˜•ì‹ì˜ ê²€ìƒ‰ ê²°ê³¼
    """
    try:
        print(f"ğŸ” ChromaDB ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        chroma_client = chromadb.PersistentClient(path="../chroma_db_news_3")
        news_collection = chroma_client.get_collection("naver_news")
        
        # ì„ë² ë”© ìƒì„±
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        query_embedding = embeddings.embed_query(query)
        
        # ê²€ìƒ‰ ì‹¤í–‰
        results = news_collection.query(
            query_embeddings=[query_embedding],
            n_results=5,
            include=["documents", "metadatas", "distances"]
        )
        
        # ê²°ê³¼ ì²˜ë¦¬
        search_results = []
        if results["documents"] and results["documents"][0]:
            for i, (doc, metadata, distance) in enumerate(zip(
                results["documents"][0],
                results["metadatas"][0], 
                results["distances"][0]
            )):
                relevance_score = 1 - distance
                
                # í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ê³„ì‚°
                keyword_bonus = 0
                if keywords:
                    try:
                        keyword_list = json.loads(keywords) if isinstance(keywords, str) else keywords
                        if isinstance(keyword_list, list):
                            for keyword in keyword_list:
                                if keyword.lower() in doc.lower():
                                    keyword_bonus += 0.1
                    except:
                        pass
                
                final_score = min(relevance_score + keyword_bonus, 1.0)
                
                search_results.append({
                    "content": doc[:300] + "..." if len(doc) > 300 else doc,
                    "title": metadata.get("title", "ì œëª© ì—†ìŒ"),
                    "url": metadata.get("url", ""),
                    "published_date": metadata.get("published_date", ""),
                    "relevance_score": final_score,
                    "rank": i + 1
                })
        
        result = {
            "success": True,
            "results_count": len(search_results),
            "results": search_results,
            "search_method": "semantic_embedding"
        }
        
        print(f"âœ… ChromaDB ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
        return json.dumps(result, ensure_ascii=False)
        
    except Exception as e:
        print(f"âŒ ChromaDB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        error_result = {
            "success": False,
            "error": str(e),
            "results": []
        }
        return json.dumps(error_result, ensure_ascii=False)

@tool  
def search_neo4j_graph_tool(query: str, keywords: Optional[str] = None) -> str:
    """
    Neo4j ê·¸ë˜í”„ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ê³„ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰í•  ì§ˆë¬¸ì´ë‚˜ ì¿¼ë¦¬
        keywords: ê²€ìƒ‰ì— ì‚¬ìš©í•  í‚¤ì›Œë“œ (JSON ë¬¸ìì—´ í˜•íƒœ)
        
    Returns:
        JSON í˜•ì‹ì˜ ê·¸ë˜í”„ ê´€ê³„ ê²°ê³¼
    """
    try:
        print(f"ğŸ”— Neo4j ê·¸ë˜í”„ ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        # Neo4j ì—°ê²°
        uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
        username = os.getenv("NEO4J_USERNAME", "neo4j")
        password = os.getenv("NEO4J_PASSWORD", "password")
        
        driver = GraphDatabase.driver(uri, auth=(username, password))
        
        # í‚¤ì›Œë“œ íŒŒì‹±
        search_keywords = []
        if keywords:
            try:
                keyword_list = json.loads(keywords) if isinstance(keywords, str) else keywords
                if isinstance(keyword_list, list):
                    search_keywords = keyword_list
            except:
                search_keywords = query.split()[:3]
        else:
            search_keywords = query.split()[:3]
        
        print(f"ğŸ”‘ ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(search_keywords)}")
        
        # ë‹¤ì–‘í•œ ê²€ìƒ‰ íŒ¨í„´ ì‹œë„
        search_patterns = [
            # íŒ¨í„´ 1: ì§ì ‘ ê´€ê³„ ê²€ìƒ‰
            """
            MATCH (a)-[r]->(b)
            WHERE any(keyword IN $keywords WHERE 
                a.name CONTAINS keyword OR b.name CONTAINS keyword)
            RETURN a.name as source, type(r) as relationship, b.name as target,
                   'direct' as pattern_type
            LIMIT 5
            """,
            # íŒ¨í„´ 2: ì¸ê³¼ê´€ê³„ ì²´ì¸
            """
            MATCH (a)-[:ì›ì¸ì´ë‹¤]->(b)-[:ê²°ê³¼ì´ë‹¤]->(c)
            WHERE any(keyword IN $keywords WHERE 
                a.name CONTAINS keyword OR b.name CONTAINS keyword OR c.name CONTAINS keyword)
            RETURN a.name as cause, b.name as intermediate, c.name as effect,
                   'causal' as pattern_type
            LIMIT 3
            """,
            # íŒ¨í„´ 3: í‚¤ì›Œë“œ ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬
            """
            MATCH (center)-[r]-(connected)
            WHERE any(keyword IN $keywords WHERE center.name CONTAINS keyword)
            RETURN center.name as center_entity, type(r) as relationship, 
                   connected.name as related_entity, 'network' as pattern_type
            LIMIT 4
            """
        ]
        
        all_results = []
        
        with driver.session() as session:
            for i, pattern in enumerate(search_patterns, 1):
                try:
                    results = session.run(pattern, {"keywords": search_keywords}).data()
                    
                    if results:
                        print(f"  ğŸ“‹ íŒ¨í„´ {i}: {len(results)}ê°œ ê´€ê³„ ë°œê²¬")
                        all_results.extend(results)
                    
                except Exception as pattern_error:
                    print(f"  âš ï¸ íŒ¨í„´ {i} ì‹¤í–‰ ì˜¤ë¥˜: {pattern_error}")
                    continue
        
        driver.close()
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        unique_results = []
        seen_relationships = set()
        
        for result in all_results:
            # ê´€ê³„ í‚¤ ìƒì„±
            if "relationship" in result:
                key = f"{result.get('source', '')}-{result['relationship']}-{result.get('target', '')}"
            elif "cause" in result:
                key = f"{result['cause']}-{result['intermediate']}-{result['effect']}"
            else:
                key = str(result)
            
            if key not in seen_relationships:
                seen_relationships.add(key)
                unique_results.append(result)
        
        final_result = {
            "success": True,
            "results_count": len(unique_results),
            "results": unique_results[:8],  # ìƒìœ„ 8ê°œë§Œ ì„ íƒ
            "search_keywords": search_keywords
        }
        
        print(f"âœ… Neo4j ê²€ìƒ‰ ì™„ë£Œ: {len(unique_results)}ê°œ ê´€ê³„")
        return json.dumps(final_result, ensure_ascii=False)
        
    except Exception as e:
        print(f"âŒ Neo4j ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        error_result = {
            "success": False,
            "error": str(e),
            "results": []
        }
        return json.dumps(error_result, ensure_ascii=False)

@tool
def synthesize_answer_tool(question: str, chroma_results: str, neo4j_results: str) -> str:
    """
    ChromaDBì™€ Neo4j ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        question: ì›ë³¸ ì§ˆë¬¸
        chroma_results: ChromaDB ê²€ìƒ‰ ê²°ê³¼ (JSON ë¬¸ìì—´)
        neo4j_results: Neo4j ê²€ìƒ‰ ê²°ê³¼ (JSON ë¬¸ìì—´)
        
    Returns:
        JSON í˜•ì‹ì˜ ìµœì¢… ë‹µë³€ê³¼ ë©”íƒ€ë°ì´í„°
    """
    try:
        print(f"ğŸ§  ë‹µë³€ í†µí•© ìƒì„± ì‹œì‘")
        
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)
        
        # ê²°ê³¼ íŒŒì‹±
        try:
            if isinstance(chroma_results, str):
                chroma_data = json.loads(chroma_results)
            elif isinstance(chroma_results, dict):
                chroma_data = chroma_results
            elif isinstance(chroma_results, list):
                chroma_data = {"results": chroma_results}
            else:
                chroma_data = {"results": []}
                
            if isinstance(neo4j_results, str):
                neo4j_data = json.loads(neo4j_results)
            elif isinstance(neo4j_results, dict):
                neo4j_data = neo4j_results
            elif isinstance(neo4j_results, list):
                neo4j_data = {"results": neo4j_results}
            else:
                neo4j_data = {"results": []}
        except:
            chroma_data = {"results": []}
            neo4j_data = {"results": []}
        
        # ë‰´ìŠ¤ ì •ë³´ ì •ë¦¬
        news_context = ""
        sources = []
        if chroma_data.get("results"):
            news_context = "\n=== ê´€ë ¨ ë‰´ìŠ¤ ì •ë³´ ===\n"
            for i, news in enumerate(chroma_data["results"][:3], 1):
                news_context += f"[ë‰´ìŠ¤ {i}] {news.get('title', 'ì œëª©ì—†ìŒ')}\n"
                news_context += f"ë‚´ìš©: {news.get('content', 'ë‚´ìš©ì—†ìŒ')}\n"
                news_context += f"ë‚ ì§œ: {news.get('published_date', 'ë‚ ì§œì—†ìŒ')}\n"
                news_context += f"ê´€ë ¨ë„: {news.get('relevance_score', 0):.3f}\n\n"
                
                if news.get('url'):
                    sources.append(news['url'])
        
        # ê·¸ë˜í”„ ê´€ê³„ ì •ë¦¬
        graph_context = ""
        if neo4j_data.get("results"):
            graph_context = "\n=== ê´€ë ¨ ê·¸ë˜í”„ ì •ë³´ ===\n"
            for i, rel in enumerate(neo4j_data["results"][:5], 1):
                if "relationship" in rel:
                    graph_context += f"[ê´€ê³„ {i}] {rel.get('source', '')} -[{rel['relationship']}]-> {rel.get('target', '')}\n"
                elif "cause" in rel:
                    graph_context += f"[ì¸ê³¼ {i}] {rel['cause']} â†’ {rel['intermediate']} â†’ {rel['effect']}\n"
                else:
                    graph_context += f"[ê¸°íƒ€ {i}] {rel}\n"
        
        # í†µí•© ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
        synthesis_prompt = f"""
ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ì™€ ê´€ê³„ ê·¸ë˜í”„ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ì •í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

{news_context}

{graph_context}

ë‹µë³€ ìƒì„± ì§€ì¹¨:
1. ë‰´ìŠ¤ ê¸°ì‚¬ì˜ êµ¬ì²´ì  ì‚¬ì‹¤ì„ ë‹µë³€ì˜ í•µì‹¬ìœ¼ë¡œ ì‚¬ìš©
2. ê·¸ë˜í”„ ê´€ê³„ë¡œ ë§¥ë½ê³¼ ë°°ê²½ ì„¤ëª… ë³´ê°•  
3. ë‚ ì§œ, ì¶œì²˜, êµ¬ì²´ì  ìˆ˜ì¹˜ ë“± ì‚¬ì‹¤ ì •ë³´ ëª…ì‹œ
4. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ ì§€ì‹ ì‚¬ìš© ê¸ˆì§€
5. ìì—°ìŠ¤ëŸ½ê³  ì²´ê³„ì ì¸ ë¬¸ë‹¨ êµ¬ì„±
6. ì •ë³´ì˜ ì‹ ë¢°ë„ì™€ í•œê³„ ëª…ì‹œ

ë‹µë³€ì„ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”:
{{
    "answer": "ìƒì„¸í•œ ë‹µë³€ ë‚´ìš©",
    "confidence": 0.0-1.0 ì‚¬ì´ì˜ ì‹ ë¢°ë„,
    "reasoning": "ë‹µë³€ ìƒì„± ê³¼ì • ì„¤ëª…",
    "limitations": "ë‹µë³€ì˜ í•œê³„ë‚˜ ì£¼ì˜ì‚¬í•­"
}}
"""
        
        config = {"callbacks": [langfuse_handler]} if langfuse_handler else {}
        response = llm.invoke(synthesis_prompt, config=config)
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            answer_data = json.loads(response.content)
        except:
            # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ êµ¬ì¡°
            answer_data = {
                "answer": response.content,
                "confidence": 0.7,
                "reasoning": "ìë™ ìƒì„±ëœ ë‹µë³€",
                "limitations": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ í˜•ì‹ ì‚¬ìš©"
            }
        
        # ìµœì¢… ê²°ê³¼ êµ¬ì„±
        final_result = {
            "success": True,
            "answer": answer_data.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"),
            "confidence_score": answer_data.get("confidence", 0.5),
            "reasoning": answer_data.get("reasoning", ""),
            "limitations": answer_data.get("limitations", ""),
            "sources": sources,
            "news_count": len(chroma_data.get("results", [])),
            "graph_relations_count": len(neo4j_data.get("results", []))
        }
        
        print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (ì‹ ë¢°ë„: {final_result['confidence_score']:.3f})")
        return json.dumps(final_result, ensure_ascii=False)
        
    except Exception as e:
        print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
        error_result = {
            "success": False,
            "error": str(e),
            "answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
            "confidence_score": 0.0
        }
        return json.dumps(error_result, ensure_ascii=False)

# Tool ë¦¬ìŠ¤íŠ¸
tools = [
    analyze_question_tool,
    search_chroma_news_tool, 
    search_neo4j_graph_tool,
    synthesize_answer_tool
]

# Agent ë…¸ë“œ í•¨ìˆ˜ë“¤
def agent_node(state: ToolAgentState):
    """LLM Agentê°€ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  ì‹¤í–‰í•˜ëŠ” ë…¸ë“œ"""
    messages = state["messages"]
    
    # Tool-callingì´ ê°€ëŠ¥í•œ LLM ìƒì„±
    llm_with_tools = ChatOpenAI(
        model="gpt-4o-mini", 
        temperature=0
    ).bind_tools(tools)
    
    # Agent ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    agent_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì´ì í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ë‹¤ìŒ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  í¬ê´„ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:

1. analyze_question_tool: ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½
2. search_chroma_news_tool: ChromaDBì—ì„œ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰
3. search_neo4j_graph_tool: Neo4jì—ì„œ ê´€ê³„ ê·¸ë˜í”„ ê²€ìƒ‰  
4. synthesize_answer_tool: ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±

ì‘ì—… ìˆœì„œ:
1. ë¨¼ì € ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”
2. ë¶„ì„ ê²°ê³¼ì— ë”°ë¼ ChromaDBì™€ Neo4jì—ì„œ ë³‘ë ¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì„¸ìš”
3. ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”
4. ë‹µë³€ì—ëŠ” ì‹ ë¢°ë„, ì¶œì²˜, í•œê³„ì ì„ í¬í•¨í•˜ì„¸ìš”

ì¤‘ìš”: ê° ë„êµ¬ì˜ ê²°ê³¼ë¥¼ í™•ì¸í•œ í›„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì§„í–‰í•˜ì„¸ìš”."""),
        ("placeholder", "{messages}")
    ])
    
    # í”„ë¡¬í”„íŠ¸ ì ìš©
    prompt_response = agent_prompt.invoke({"messages": messages})
    
    # LLM ì‘ë‹µ ìƒì„±
    config = {"callbacks": [langfuse_handler]} if langfuse_handler else {}
    response = llm_with_tools.invoke(prompt_response.messages, config=config)
    
    return {"messages": [response]}

def tool_node(state: ToolAgentState):
    """ë„êµ¬ ì‹¤í–‰ ë…¸ë“œ"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # ToolNodeë¥¼ ì‚¬ìš©í•˜ì—¬ ë„êµ¬ í˜¸ì¶œ ì‹¤í–‰
    tool_executor = ToolNode(tools)
    return tool_executor.invoke(state)

def should_continue(state: ToolAgentState):
    """ë‹¤ìŒ ë‹¨ê³„ ê²°ì • í•¨ìˆ˜"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ë„êµ¬ í˜¸ì¶œì„ í¬í•¨í•˜ë©´ ë„êµ¬ ì‹¤í–‰
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "tools"
    # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œ
    else:
        return "end"

class ToolBasedHybridRAGAgent:
    """Tool-calling ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ RAG Agent"""
    
    def __init__(self):
        self.checkpointer = InMemorySaver()
        self.graph = self._create_agent_graph()
        
    def _create_agent_graph(self):
        """Agent ê·¸ë˜í”„ ìƒì„±"""
        # StateGraph ì´ˆê¸°í™”
        workflow = StateGraph(ToolAgentState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("agent", agent_node)
        workflow.add_node("tools", tool_node)
        
        # ì‹œì‘ì  ì„¤ì •
        workflow.add_edge(START, "agent")
        
        # ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
        workflow.add_conditional_edges(
            "agent",
            should_continue,
            {
                "tools": "tools",
                "end": END
            }
        )
        
        # ë„êµ¬ ì‹¤í–‰ í›„ ë‹¤ì‹œ agentë¡œ
        workflow.add_edge("tools", "agent")
        
        # ê·¸ë˜í”„ ì»´íŒŒì¼
        return workflow.compile(checkpointer=self.checkpointer)
    
    def run(self, question: str, thread_id: str = None):
        """Agent ì‹¤í–‰"""
        if thread_id is None:
            thread_id = f"session_{int(time.time())}"
        
        print(f"ğŸš€ Tool-based Hybrid RAG Agent ì‹œì‘")
        print(f"â“ ì§ˆë¬¸: {question}")
        print("=" * 60)
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = {
            "messages": [HumanMessage(content=question)],
            "question": question,
            "search_results": {},
            "final_answer": "",
            "confidence_score": 0.0,
            "sources": [],
            "execution_log": []
        }
        
        # ì„¤ì •
        config = {
            "configurable": {"thread_id": thread_id},
            "callbacks": [langfuse_handler] if langfuse_handler else []
        }
        
        try:
            start_time = time.time()
            
            # ê·¸ë˜í”„ ì‹¤í–‰
            result = self.graph.invoke(initial_state, config=config)
            
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ íŒŒì‹±
            final_result = self._parse_final_result(result, execution_time)
            
            print(f"\nğŸ‰ ì‹¤í–‰ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {execution_time:.2f}ì´ˆ)")
            print("=" * 60)
            
            return final_result
            
        except Exception as e:
            print(f"âŒ Agent ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            import traceback
            print(traceback.format_exc())
            
            return {
                "success": False,
                "error": str(e),
                "question": question,
                "final_answer": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
                "confidence_score": 0.0,
                "sources": [],
                "execution_time": 0
            }
    
    def _parse_final_result(self, result, execution_time):
        """ê²°ê³¼ íŒŒì‹± ë° ì •ë¦¬"""
        messages = result.get("messages", [])
        
        # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ì—ì„œ ìµœì¢… ë‹µë³€ ì°¾ê¸°
        final_answer = ""
        confidence_score = 0.0
        sources = []
        
        # Tool í˜¸ì¶œ ê²°ê³¼ë“¤ ìˆ˜ì§‘
        tool_results = []
        for message in messages:
            if hasattr(message, 'content') and message.content:
                # synthesize_answer_toolì˜ ê²°ê³¼ì¸ì§€ í™•ì¸
                if isinstance(message.content, str):
                    try:
                        # JSON íŒŒì‹± ì‹œë„
                        parsed_content = json.loads(message.content)
                        if isinstance(parsed_content, dict) and "answer" in parsed_content:
                            final_answer = parsed_content.get("answer", "")
                            confidence_score = parsed_content.get("confidence_score", 0.0)
                            sources = parsed_content.get("sources", [])
                            break
                    except:
                        # JSONì´ ì•„ë‹Œ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        if len(message.content) > 50:  # ì¶©ë¶„íˆ ê¸´ ë‹µë³€ì¸ ê²½ìš°
                            final_answer = message.content
                            confidence_score = 0.7
        
        # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ê°€ ìµœì¢… ë‹µë³€ì¸ ê²½ìš°
        if not final_answer:
            for message in reversed(messages):
                if hasattr(message, 'content') and hasattr(message, 'type'):
                    if message.type == 'ai' and message.content:
                        final_answer = message.content
                        confidence_score = 0.5
                        break
        
        if not final_answer:
            final_answer = "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        # Tool ì‚¬ìš© í†µê³„
        tool_calls_count = 0
        used_tools = set()
        
        for message in messages:
            if hasattr(message, 'tool_calls') and message.tool_calls:
                tool_calls_count += len(message.tool_calls)
                for tool_call in message.tool_calls:
                    used_tools.add(tool_call['name'])
        
        return {
            "success": True,
            "question": result.get("question", ""),
            "final_answer": final_answer,
            "confidence_score": confidence_score,
            "sources": sources,
            "execution_time": execution_time,
            "tool_calls_count": tool_calls_count,
            "used_tools": list(used_tools),
            "message_count": len(messages)
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Tool-based Hybrid RAG Agent ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # Agent ì´ˆê¸°í™”
    agent = ToolBasedHybridRAGAgent()
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ìµœê·¼ ì •ì¹˜ì  ì‚¬ê±´ë“¤ì´ êµ­ì • ìš´ì˜ì— ë¯¸ì¹œ ì˜í–¥ì€?",
        "ìµœê·¼ í•œêµ­ ê²½ì œ ìƒí™©ì€ ì–´ë–¤ê°€ìš”?",
        "ì½”ìŠ¤í”¼ ì§€ìˆ˜ê°€ ìµœê·¼ ìƒìŠ¹í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*10} ì§ˆë¬¸ {i} {'='*10}")
        
        try:
            result = agent.run(question, thread_id=f"test_session_{i}")
            
            print(f"\nğŸ“‹ ìµœì¢… ë‹µë³€:")
            print(result["final_answer"])
            
            print(f"\nğŸ“Š ì‹¤í–‰ í†µê³„:")
            print(f"  ğŸ¯ ì‹ ë¢°ë„: {result['confidence_score']:.3f}")
            print(f"  ğŸ”§ ì‚¬ìš©ëœ ë„êµ¬: {', '.join(result.get('used_tools', []))}")
            print(f"  ğŸ“ ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜: {result.get('tool_calls_count', 0)}íšŒ")
            print(f"  ğŸ’¬ ë©”ì‹œì§€ ìˆ˜: {result.get('message_count', 0)}ê°œ")
            print(f"  â±ï¸ ì‹¤í–‰ ì‹œê°„: {result['execution_time']:.2f}ì´ˆ")
            
            if result.get("sources"):
                print(f"\nğŸ” ì°¸ê³  ì¶œì²˜:")
                for j, source in enumerate(result["sources"][:3], 1):
                    print(f"  {j}. {source}")
                    
        except Exception as e:
            print(f"âŒ ì§ˆë¬¸ {i} ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        
        print("\n" + "=" * 60)
        
        # ë‹¤ìŒ ì§ˆë¬¸ ì „ ì ì‹œ ëŒ€ê¸°
        if i < len(test_questions):
            print("ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™ ì¤‘...")
            time.sleep(1)

if __name__ == "__main__":
    main() 